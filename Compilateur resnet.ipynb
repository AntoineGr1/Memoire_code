{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolution\n",
    "class Convolution:\n",
    "    def __init__(self, name, kernel, padding, stride, nb_filter, fct_activation):\n",
    "        self.name = name\n",
    "        self.kernel = kernel\n",
    "        self.padding = padding\n",
    "        self.stride = stride\n",
    "        self.nb_filter = nb_filter\n",
    "        self.fct_activation = fct_activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input\n",
    "class InputLayer:\n",
    "    def __init__(self, shape):\n",
    "        self.name = \"I\"\n",
    "        self.shape = shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pooling Avg/Max\n",
    "class Pooling:\n",
    "    def __init__(self, name, op, kernel=2, padding=\"valid\", stride=None):\n",
    "        self.name = name\n",
    "        self.op = op # is the operation wanted (avg/max)\n",
    "        self.kernel = kernel\n",
    "        self.padding = padding\n",
    "        self.stride = stride"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class Flatten\n",
    "class Flatten:\n",
    "    def __init__(self):\n",
    "        self.name = \"f\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dense --> Fully connected layer\n",
    "class Dense:\n",
    "    def __init__(self, name, nb_neurones, fct_activation):\n",
    "        self.name = name\n",
    "        self.nb_neurones = nb_neurones\n",
    "        self.fct_activation = fct_activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read json file \n",
    "# return list of layer + graph\n",
    "def readJsonFile(json_file):\n",
    "    with open(json_file) as archi_json:\n",
    "        file = json.load(archi_json)\n",
    "    \n",
    "    archi =file[0]['archi']\n",
    "    G_json = file[0]['graph']\n",
    "    \n",
    "    # instantiate class\n",
    "    list_layer = []\n",
    "    for layer in archi:\n",
    "        parameters = layer[\"parameters\"] # get parameters\n",
    "        # if is Input class\n",
    "        if(layer[\"class\"] == \"InputLayer\"):\n",
    "            # instantiate class and add into the list\n",
    "            list_layer.append(InputLayer(parameters[\"shape\"]))\n",
    "\n",
    "        \n",
    "        # if is Pooling class\n",
    "        elif(layer[\"class\"] == \"Pooling\"):\n",
    "            try:\n",
    "                padding = int(parameters[\"padding\"])\n",
    "            except:\n",
    "                padding = \"'\" + parameters[\"padding\"] + \"'\"\n",
    "                \n",
    "            list_layer.append(Pooling(parameters['name'],\n",
    "                                      parameters[\"op\"],\n",
    "                                      parameters[\"kernel\"],\n",
    "                                      padding,\n",
    "                                      parameters[\"stride\"]))\n",
    "            \n",
    "        # if is Convolution class\n",
    "        elif(layer[\"class\"] == \"Convolution\"):\n",
    "            try:\n",
    "                padding = int(parameters[\"padding\"])\n",
    "            except:\n",
    "                padding = \"'\" + parameters[\"padding\"] + \"'\"\n",
    "                \n",
    "                \n",
    "            list_layer.append(Convolution(parameters[\"name\"],\n",
    "                                          parameters[\"kernel\"],\n",
    "                                          padding,\n",
    "                                          parameters[\"stride\"],\n",
    "                                          parameters[\"nb_filter\"],\n",
    "                                          parameters[\"fct_activation\"]))\n",
    "            \n",
    "        # if is Flatten class\n",
    "        elif(layer[\"class\"] == \"Flatten\"):\n",
    "            list_layer.append(Flatten())\n",
    "        \n",
    "        # if is Dense class\n",
    "        elif(layer[\"class\"] == \"Dense\"):\n",
    "            list_layer.append(Dense(parameters[\"name\"],\n",
    "                                    parameters[\"nb_neurones\"],\n",
    "                                    parameters[\"fct_activation\"]))\n",
    "\n",
    "        else : print(\"Error\")\n",
    "    return list_layer, G_json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return import of the py file\n",
    "def writeImport():\n",
    "    return \"\"\"\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential, Model,load_model\n",
    "from tensorflow.keras.layers import Input, Add, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPooling2D, GlobalMaxPooling2D,MaxPool2D\n",
    "from tensorflow.keras.initializers import glorot_uniform\n",
    "from tensorflow.keras.utils import plot_model\n",
    "import sys\n",
    "import traceback\n",
    "import csv\n",
    "from time import time\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write mnist data set\n",
    "def writeMnistDataset():\n",
    "    return \"\"\"(train_x, train_y), (test_x, test_y) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# normaliser les pixel 0-255 -> 0-1\n",
    "train_x = train_x / 255.0\n",
    "test_x = test_x / 255.0\n",
    "\n",
    "train_x = tf.expand_dims(train_x, 3)\n",
    "test_x = tf.expand_dims(test_x, 3)\n",
    "\n",
    "val_x = train_x[:5000]\n",
    "val_y = train_y[:5000]\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write cifar data set\n",
    "def writecifar10Dataset():\n",
    "    return \"\"\"\n",
    "# load dataset\n",
    "(train_x, train_y), (test_x, test_y) = keras.datasets.cifar10.load_data()\n",
    "\n",
    "# normalize to range 0-1\n",
    "train_x = train_x / 255.0\n",
    "test_x = test_x / 255.0\n",
    "\n",
    "val_x = train_x[:5000]\n",
    "val_y = train_y[:5000]\n",
    "    \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write function for identity block\n",
    "def write_identity_block():\n",
    "    return \"\"\"\n",
    "    def id_block(X, f, filters):\n",
    "   \n",
    "        X_shortcut = X\n",
    "\n",
    "        X = Conv2D(filters=filters, kernel_size=(1, 1), strides=(1, 1), padding='same', kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "        # X = BatchNormalization(axis=3)(X)\n",
    "        X = Activation('relu')(X)\n",
    "\n",
    "\n",
    "        X = Conv2D(filters=filters, kernel_size=(f, f), strides=(1, 1), padding='same', kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "        # X = BatchNormalization(axis=3)(X)\n",
    "\n",
    "        X = Add()([X, X_shortcut])# SKIP Connection\n",
    "        X = Activation('relu')(X)\n",
    "\n",
    "        return X\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_conv_block():\n",
    "    return\"\"\"\n",
    "    def conv_block(X, f, filters, s=2):\n",
    "    \n",
    "        X_shortcut = X\n",
    "\n",
    "        X = Conv2D(filters=filters, kernel_size=(1, 1), strides=(s, s), padding='valid', kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "        # X = BatchNormalization(axis=3)(X)\n",
    "        X = Activation('relu')(X)\n",
    "\n",
    "        X = Conv2D(filters=filters, kernel_size=(f, f), strides=(1, 1), padding='same', kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "        # X = BatchNormalization(axis=3)(X)\n",
    "\n",
    "        X_shortcut = Conv2D(filters=filters, kernel_size=(1, 1), strides=(s, s), padding='valid', kernel_initializer=glorot_uniform(seed=0))(X_shortcut)\n",
    "        # X_shortcut = BatchNormalization(axis=3)(X_shortcut)\n",
    "\n",
    "\n",
    "        X = Add()([X, X_shortcut])\n",
    "        X = Activation('relu')(X)\n",
    "\n",
    "        return X\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return string of the wanted layer\n",
    "def write_layer(layer):\n",
    "    \n",
    "    # if is Intput\n",
    "    if isinstance(layer, InputLayer):\n",
    "        return \"        X_input = X = Input({})\\n\".format(\n",
    "                layer.shape)\n",
    "        \n",
    "            \n",
    "    # if is Convolution\n",
    "    elif isinstance(layer, Convolution):\n",
    "        return \"        X = Conv2D({}, kernel_size={}, strides={}, activation='{}', padding={})(X)\\n\".format(\n",
    "                layer.nb_filter,\n",
    "                layer.kernel,\n",
    "                layer.stride,\n",
    "                layer.fct_activation,\n",
    "                layer.padding)\n",
    "            \n",
    "    # if is Pooling\n",
    "    elif isinstance(layer, Pooling):\n",
    "        if(layer.op == \"avg\"): # avg Pooling \n",
    "            return \"        X = AveragePooling2D(pool_size={}, strides={}, padding={})(X)\\n\".format(\n",
    "                    layer.kernel,\n",
    "                    layer.stride,\n",
    "                    layer.padding)\n",
    "                \n",
    "        else : # Max Pooling\n",
    "            return  \"        X = MaxPooling2D(pool_size={}, strides={}, padding={})(X)\\n\".format(\n",
    "                    layer.kernel,\n",
    "                    layer.stride,\n",
    "                    layer.padding)\n",
    "    # Not possible\n",
    "    else : print(\"Not Possible\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "def end_CNN(archi, nodes, i, j, str_model_cnn):\n",
    "    # if is Dense (aka Fully connected layer)\n",
    "    i+=1\n",
    "    while((i < len(nodes)) and (j <= len(archi))):\n",
    "        print(nodes[i])\n",
    "        print(archi[j].name)\n",
    "        layer = archi[j]\n",
    "        if isinstance(layer, Dense):\n",
    "            str_model_cnn += \"    head_model = Dense({}, activation='{}')(head_model)\\n\".format(\n",
    "                            layer.nb_neurones, \n",
    "                            layer.fct_activation)\n",
    "            i+=1\n",
    "            j+=1\n",
    "        # if is flatten\n",
    "        elif isinstance(layer, Flatten):\n",
    "            str_model_cnn += \"    head_model = Flatten()(head_model)\\n\"\n",
    "            i+=1\n",
    "            j+=1\n",
    "        # Not possible\n",
    "        else : print(\"Not Possible\")\n",
    "    return i, j, str_model_cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_CNN(archi, g_json):\n",
    "    nodes = g_json['nodes']\n",
    "    resnet_module = False\n",
    "    str_model_cnn = \"def ResNet():\\n\"\n",
    "    i=0\n",
    "    j=0\n",
    "    print(len(nodes))\n",
    "    print(len(archi))\n",
    "    \n",
    "    while ((i < len(nodes)) and (j <= len(archi))):\n",
    "        if(nodes[i]['id'] == \"add0\"): # block resnet begin\n",
    "            resnet_module = True\n",
    "            \n",
    "        if resnet_module == False:\n",
    "            str_model_cnn += write_layer(archi[i])\n",
    "            i+=1\n",
    "            j+=1\n",
    "        else :\n",
    "            if(( (i+4) < len(nodes)) and (\"add\" in nodes[i+3]['id'])):\n",
    "                str_model_cnn += \"        X = id_block(X, {}, {})\\n\".format(archi[j+1].kernel, archi[j].nb_filter)\n",
    "                i += 3\n",
    "                j += 2\n",
    "            elif (( (i+4) < len(nodes)) and (\"add\" in nodes[i+4]['id'])):\n",
    "                str_model_cnn += \"        X = conv_block(X, {}, {}, {})\\n\".format(archi[j+1].kernel, archi[j].nb_filter, archi[j].stride)\n",
    "                i += 4\n",
    "                j += 3\n",
    "            else:\n",
    "                str_model_cnn += \"        model = Model(inputs=X_input, outputs=X, name='ResNet18')\\n\"\n",
    "                str_model_cnn += \"        return model\\n\\n\"\n",
    "                str_model_cnn += \"    Input = ResNet()\\n\"\n",
    "                str_model_cnn += \"    head_model = Input.output\\n\"\n",
    "                i, j, str_model_cnn = end_CNN(archi, nodes, i, j, str_model_cnn)\n",
    "\n",
    "    str_model_cnn += \"    model = Model(inputs=Input.input, outputs=head_model)\\n\"\n",
    "    return str_model_cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_py_file(json_file):\n",
    "    s = json_file.split(\".\")\n",
    "    file_name = \"archi_resnet_test\" #s[0].split(\"/\")[1]\n",
    "    architecture, g_json = readJsonFile(json_file)\n",
    "    \n",
    "    # python directory\n",
    "    py_dir = \"architecture_py/\"\n",
    "    \n",
    "    # log directory\n",
    "    log_dir = \"../architecture_log/\"\n",
    "    \n",
    "    # png directory\n",
    "    png_dir = \"../architecture_img/\"\n",
    "    \n",
    "    # reset file\n",
    "    file_py = open(py_dir + file_name + \".py\", \"w\")\n",
    "    file_py.close()\n",
    "    \n",
    "    file_py = open(py_dir + file_name + \".py\", \"a\") # Open file in writting (a --> append)\n",
    "    \n",
    "    # write import\n",
    "    file_py.write(writeImport())\n",
    "    \n",
    "    # write train/test data \n",
    "    file_py.write(writeMnistDataset())\n",
    "    \n",
    "    file_py.write(\"\"\"\n",
    "\n",
    "# init training time\n",
    "training_time = 0\n",
    "# init result test/train\n",
    "test_result_loss = \"\"\n",
    "test_result_acc = \"\"\n",
    "\n",
    "train_result_loss = \"\"\n",
    "train_result_acc = \"\"\n",
    "\n",
    "nb_layers = \"not build\"\n",
    "\n",
    "\"\"\")\n",
    "    \n",
    "    # try\n",
    "    file_py.write(\"\"\"try:\n",
    "    \"\"\")\n",
    "    \n",
    "    # write function for id block\n",
    "    file_py.write(write_identity_block())\n",
    "    \n",
    "    # write function for conv block\n",
    "    file_py.write(write_conv_block())\n",
    "    \n",
    "    # write architecture model\n",
    "    file_py.write(create_CNN(architecture, g_json))\n",
    "    \n",
    "    \n",
    "    # write : create png of the model\n",
    "    file_py.write(\"    plot_model(model, show_shapes=True, to_file=\\\"%s\\\")\\n\" % (png_dir+file_name+\".png\"))\n",
    "    \n",
    "    # write compiler\n",
    "    file_py.write(\"\"\"    model.compile(optimizer='adam', loss=keras.losses.sparse_categorical_crossentropy, metrics=['accuracy'])\\n\"\"\")\n",
    "    \n",
    "    # write way for time computation\n",
    "    file_py.write(\"\"\"    start = time()\\n\"\"\")\n",
    "    \n",
    "    # write model training\n",
    "    file_py.write(\"\"\"    model.fit(train_x, train_y, epochs=5, validation_data=(val_x, val_y))\\n\"\"\")\n",
    "    \n",
    "    # write time computation\n",
    "    file_py.write(\"\"\"    training_time = time()-start\\n\"\"\")\n",
    "    \n",
    "    # write model evaluation\n",
    "    file_py.write(\"\"\"    print(model.evaluate(test_x, test_y))\\n\"\"\")\n",
    "    \n",
    "    \n",
    "    # all is great\n",
    "    log_file = log_dir + file_name +\".log\"\n",
    "    file_py.write(\"\"\"\n",
    "    log_file = open(\\\"\"\"\" + log_file + \"\"\"\\\" , \"w\")\n",
    "    \n",
    "    # save test result\n",
    "    log_file.write('test result : ' + str(model.evaluate(test_x, test_y)))\n",
    "    test_result_loss = model.evaluate(test_x, test_y)[0]\n",
    "    test_result_acc = model.evaluate(test_x, test_y)[1]\n",
    "    \n",
    "    # save train result\n",
    "    log_file.write('train result : ' + str(model.evaluate(test_x, test_y)))\n",
    "    train_result_loss = model.evaluate(train_x, train_y)[0]\n",
    "    train_result_acc = model.evaluate(train_x, train_y)[1]\n",
    "    \n",
    "    print('OK: file \"\"\" + log_file +\"\"\" has been create')\n",
    "    \n",
    "    nb_layers = len(model.layers)\n",
    "    log_file.close()\n",
    "\"\"\")\n",
    "    \n",
    "    # something go wrong \n",
    "    error_file = log_dir + file_name + \"_error.log\"\n",
    "    file_py.write(\"\"\"except:\n",
    "    print('error: file \"\"\" + error_file +\"\"\" has been create')\n",
    "    error_file = open(\\\"\"\"\" + error_file + \"\"\"\\\" , \"w\")\n",
    "    traceback.print_exc(file=error_file)\n",
    "    result_loss = \"Error\"\n",
    "    result_acc = \"Error\"\n",
    "    error_file.close()\n",
    "\"\"\")\n",
    "    \n",
    "    file_py.write(\"\"\"finally:\n",
    "    file = open('../architecture_results_resnet.csv', 'a', newline ='')\n",
    "    with file: \n",
    "\n",
    "        # identifying header   \n",
    "        header = ['file_name', 'training_time(s)', 'test_result_loss', 'test_result_acc', 'train_result_acc', 'train_result_loss', 'nb_layers'] \n",
    "        writer = csv.DictWriter(file, fieldnames = header) \n",
    "      \n",
    "        # writing data row-wise into the csv file \n",
    "        writer.writeheader() \n",
    "        writer.writerow({'file_name' : '\"\"\"+ file_name + \"\"\"',  \n",
    "                         'training_time(s)': training_time,  \n",
    "                         'test_result_loss': test_result_loss,\n",
    "                         'test_result_acc': test_result_loss,\n",
    "                         'train_result_acc': train_result_acc,\n",
    "                         'train_result_loss': train_result_loss,\n",
    "                         'nb_layers': nb_layers}) \n",
    "        print('add line into architecture_results.csv')\n",
    "    file.close()\n",
    "    \"\"\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # close\n",
    "    file_py.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"architecture_json/\"\n",
    "archi, G_json = readJsonFile(directory+\"archi_resnet_test.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'directed': True,\n",
       " 'multigraph': False,\n",
       " 'graph': {},\n",
       " 'nodes': [{'id': 'I'},\n",
       "  {'id': 'C1'},\n",
       "  {'id': 'P1'},\n",
       "  {'id': 'add0'},\n",
       "  {'id': 'C2'},\n",
       "  {'id': 'C3'},\n",
       "  {'id': 'add1'},\n",
       "  {'id': 'C4'},\n",
       "  {'id': 'C5'},\n",
       "  {'id': 'C6'},\n",
       "  {'id': 'add2'},\n",
       "  {'id': 'C7'},\n",
       "  {'id': 'C8'},\n",
       "  {'id': 'add3'},\n",
       "  {'id': 'C9'},\n",
       "  {'id': 'C10'},\n",
       "  {'id': 'C11'},\n",
       "  {'id': 'add4'},\n",
       "  {'id': 'F'},\n",
       "  {'id': 'D1'},\n",
       "  {'id': 'D2'},\n",
       "  {'id': 'D3'},\n",
       "  {'id': 'D4'},\n",
       "  {'id': 'D5'}],\n",
       " 'links': [{'source': 'I', 'target': 'C1'},\n",
       "  {'source': 'C1', 'target': 'P1'},\n",
       "  {'source': 'P1', 'target': 'add0'},\n",
       "  {'source': 'add0', 'target': 'C2'},\n",
       "  {'source': 'add0', 'target': 'add1'},\n",
       "  {'source': 'C2', 'target': 'C3'},\n",
       "  {'source': 'C3', 'target': 'add1'},\n",
       "  {'source': 'add1', 'target': 'C4'},\n",
       "  {'source': 'add1', 'target': 'C6'},\n",
       "  {'source': 'C4', 'target': 'C4'},\n",
       "  {'source': 'C4', 'target': 'C5'},\n",
       "  {'source': 'C5', 'target': 'add2'},\n",
       "  {'source': 'C6', 'target': 'add2'},\n",
       "  {'source': 'add2', 'target': 'C7'},\n",
       "  {'source': 'add2', 'target': 'add3'},\n",
       "  {'source': 'C7', 'target': 'C7'},\n",
       "  {'source': 'C7', 'target': 'C8'},\n",
       "  {'source': 'C8', 'target': 'add3'},\n",
       "  {'source': 'add3', 'target': 'C9'},\n",
       "  {'source': 'add3', 'target': 'C11'},\n",
       "  {'source': 'C9', 'target': 'C9'},\n",
       "  {'source': 'C9', 'target': 'C10'},\n",
       "  {'source': 'C10', 'target': 'add4'},\n",
       "  {'source': 'C11', 'target': 'add4'},\n",
       "  {'source': 'add4', 'target': 'F'},\n",
       "  {'source': 'F', 'target': 'D1'},\n",
       "  {'source': 'D1', 'target': 'D2'},\n",
       "  {'source': 'D2', 'target': 'D3'},\n",
       "  {'source': 'D3', 'target': 'D4'},\n",
       "  {'source': 'D4', 'target': 'D5'}]}"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n",
      "19\n",
      "{'id': 'F'}\n",
      "f\n",
      "{'id': 'D1'}\n",
      "D1\n",
      "{'id': 'D2'}\n",
      "D2\n",
      "{'id': 'D3'}\n",
      "D3\n",
      "{'id': 'D4'}\n",
      "D4\n",
      "{'id': 'D5'}\n",
      "D5\n"
     ]
    }
   ],
   "source": [
    "string = create_CNN(archi, G_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"def ResNet():\\n        X_input = X = Input([32, 32, 3])\\n        X = Conv2D(18, kernel_size=7, strides=2, activation='selu', padding='valid')(X)\\n        X = MaxPooling2D(pool_size=3, strides=2, padding='valid')(X)\\n        X = id_block(X, 3, 18)\\n        X = conv_block(X, 3, 36, 2)\\n        X = id_block(X, 3, 36)\\n        X = conv_block(X, 3, 72, 2)\\n        model = Model(inputs=X_input, outputs=X, name='ResNet18')\\n        return model\\n\\n    Input = ResNet()\\n    head_model = Input.output\\n    head_model = Flatten()(head_model)\\n    head_model = Dense(33, activation='relu')(head_model)\\n    head_model = Dense(28, activation='tanh')(head_model)\\n    head_model = Dense(22, activation='tanh')(head_model)\\n    head_model = Dense(14, activation='tanh')(head_model)\\n    head_model = Dense(10, activation='softmax')(head_model)\\n    model = Model(inputs=Input.input, outputs=head_model)\\n\""
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n",
      "17\n",
      "{'id': 'F'}\n",
      "f\n",
      "{'id': 'D1'}\n",
      "D1\n",
      "{'id': 'D2'}\n",
      "D2\n",
      "{'id': 'D3'}\n",
      "D3\n"
     ]
    }
   ],
   "source": [
    "create_py_file(directory+\"archi_resnet_test.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
