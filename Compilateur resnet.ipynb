{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "\n",
    "import sys\n",
    "sys.path.append('../PlotNeuralNet-master/')\n",
    "from pycore.tikzeng import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolution\n",
    "class Convolution:\n",
    "    def __init__(self, kernel, padding, stride, nb_filter, fct_activation):\n",
    "        self.kernel = kernel\n",
    "        self.padding = padding\n",
    "        self.stride = stride\n",
    "        self.nb_filter = nb_filter\n",
    "        self.fct_activation = fct_activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input\n",
    "class InputLayer:\n",
    "    def __init__(self, shape):\n",
    "        self.shape = shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pooling Avg/Max\n",
    "class Pooling:\n",
    "    def __init__(self, op, kernel=2, padding=\"valid\", stride=None):\n",
    "        self.op = op # is the operation wanted (avg/max)\n",
    "        self.kernel = kernel\n",
    "        self.padding = padding\n",
    "        self.stride = stride"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class Flatten\n",
    "class Flatten:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dense --> Fully connected layer\n",
    "class Dense:\n",
    "    def __init__(self, nb_neurones, fct_activation):\n",
    "        self.nb_neurones = nb_neurones\n",
    "        self.fct_activation = fct_activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identity block \n",
    "class IdBlock:\n",
    "    def __init__(self, kernel, padding, nb_filter):\n",
    "        self.kernel = kernel\n",
    "        self.padding = padding\n",
    "        self.nb_filter = nb_filter\n",
    "        self.stride = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convolution block\n",
    "class ConvBlock:\n",
    "    def __init__(self, kernel, padding, nb_filter, stride):\n",
    "        self.kernel = kernel\n",
    "        self.nb_filter = nb_filter\n",
    "        self.padding = padding\n",
    "        self.stride = stride"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read json file \n",
    "# return list of layer + graph\n",
    "def readJsonFile(json_file):\n",
    "    with open(json_file) as archi_json:\n",
    "        file = json.load(archi_json)\n",
    "    \n",
    "    \n",
    "    # instantiate class\n",
    "    list_layer = []\n",
    "    for layer in file:\n",
    "        parameters = layer[\"parameters\"] # get parameters\n",
    "        # if is Input class\n",
    "        if(layer[\"class\"] == \"InputLayer\"):\n",
    "            # instantiate class and add into the list\n",
    "            list_layer.append(InputLayer(parameters[\"shape\"]))\n",
    "\n",
    "        \n",
    "        # if is Pooling class\n",
    "        elif(layer[\"class\"] == \"Pooling\"):\n",
    "            try:\n",
    "                padding = int(parameters[\"padding\"])\n",
    "            except:\n",
    "                padding = \"'\" + parameters[\"padding\"] + \"'\"\n",
    "                \n",
    "            list_layer.append(Pooling(parameters[\"op\"],\n",
    "                                      parameters[\"kernel\"],\n",
    "                                      padding,\n",
    "                                      parameters[\"stride\"]))\n",
    "            \n",
    "        # if is Convolution class\n",
    "        elif(layer[\"class\"] == \"Convolution\"):\n",
    "            try:\n",
    "                padding = int(parameters[\"padding\"])\n",
    "            except:\n",
    "                padding = \"'\" + parameters[\"padding\"] + \"'\"\n",
    "                \n",
    "                \n",
    "            list_layer.append(Convolution(parameters[\"kernel\"],\n",
    "                                          padding,\n",
    "                                          parameters[\"stride\"],\n",
    "                                          parameters[\"nb_filter\"],\n",
    "                                          parameters[\"fct_activation\"]))\n",
    "            \n",
    "        # if is Flatten class\n",
    "        elif(layer[\"class\"] == \"Flatten\"):\n",
    "            list_layer.append(Flatten())\n",
    "        \n",
    "        # if is Dense class\n",
    "        elif(layer[\"class\"] == \"Dense\"):\n",
    "            list_layer.append(Dense(parameters[\"nb_neurones\"],\n",
    "                                    parameters[\"fct_activation\"]))\n",
    "        \n",
    "        elif(layer[\"class\"] == \"IdBlock\"):\n",
    "            list_layer.append(IdBlock(parameters[\"kernel\"],\n",
    "                                    parameters[\"padding\"],\n",
    "                                    parameters[\"nb_filter\"]))\n",
    "            \n",
    "        elif(layer[\"class\"] == \"ConvBlock\"):\n",
    "            list_layer.append(ConvBlock(parameters[\"kernel\"],\n",
    "                                        parameters[\"padding\"],\n",
    "                                        parameters[\"nb_filter\"],\n",
    "                                        parameters[\"stride\"]))\n",
    "\n",
    "        else : print(\"Error\")\n",
    "    return list_layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return import of the py file\n",
    "def writeImport():\n",
    "    return \"\"\"\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential, Model,load_model\n",
    "from tensorflow.keras.layers import Input, Add, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPooling2D, GlobalMaxPooling2D,MaxPool2D\n",
    "from tensorflow.keras.initializers import glorot_uniform\n",
    "from tensorflow.keras.utils import plot_model\n",
    "import sys\n",
    "import traceback\n",
    "import csv\n",
    "from time import time\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write mnist data set\n",
    "def writeMnistDataset():\n",
    "    return \"\"\"(train_x, train_y), (test_x, test_y) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# normaliser les pixel 0-255 -> 0-1\n",
    "train_x = train_x / 255.0\n",
    "test_x = test_x / 255.0\n",
    "\n",
    "train_x = tf.expand_dims(train_x, 3)\n",
    "test_x = tf.expand_dims(test_x, 3)\n",
    "\n",
    "val_x = train_x[:5000]\n",
    "val_y = train_y[:5000]\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write cifar data set\n",
    "def writecifar10Dataset():\n",
    "    return \"\"\"\n",
    "# load dataset\n",
    "(train_x, train_y), (test_x, test_y) = keras.datasets.cifar10.load_data()\n",
    "\n",
    "# normalize to range 0-1\n",
    "train_x = train_x / 255.0\n",
    "test_x = test_x / 255.0\n",
    "\n",
    "val_x = train_x[:5000]\n",
    "val_y = train_y[:5000]\n",
    "    \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write function for identity block\n",
    "def write_identity_block():\n",
    "    return \"\"\"\n",
    "    def id_block(X, f, filters):\n",
    "   \n",
    "        X_shortcut = X\n",
    "\n",
    "        X = Conv2D(filters=filters, kernel_size=(1, 1), strides=(1, 1), padding='same', kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "        # X = BatchNormalization(axis=3)(X)\n",
    "        X = Activation('relu')(X)\n",
    "\n",
    "\n",
    "        X = Conv2D(filters=filters, kernel_size=(f, f), strides=(1, 1), padding='same', kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "        # X = BatchNormalization(axis=3)(X)\n",
    "\n",
    "        X = Add()([X, X_shortcut])# SKIP Connection\n",
    "        X = Activation('relu')(X)\n",
    "\n",
    "        return X\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_conv_block():\n",
    "    return\"\"\"\n",
    "    def conv_block(X, f, filters, s=2):\n",
    "    \n",
    "        X_shortcut = X\n",
    "\n",
    "        X = Conv2D(filters=filters, kernel_size=(1, 1), strides=(s, s), padding='valid', kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "        # X = BatchNormalization(axis=3)(X)\n",
    "        X = Activation('relu')(X)\n",
    "\n",
    "        X = Conv2D(filters=filters, kernel_size=(f, f), strides=(1, 1), padding='same', kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "        # X = BatchNormalization(axis=3)(X)\n",
    "\n",
    "        X_shortcut = Conv2D(filters=filters, kernel_size=(1, 1), strides=(s, s), padding='valid', kernel_initializer=glorot_uniform(seed=0))(X_shortcut)\n",
    "        # X_shortcut = BatchNormalization(axis=3)(X_shortcut)\n",
    "\n",
    "\n",
    "        X = Add()([X, X_shortcut])\n",
    "        X = Activation('relu')(X)\n",
    "\n",
    "        return X\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return string of the wanted layer\n",
    "def write_layer(layer):\n",
    "    \n",
    "    # if is Intput\n",
    "    if isinstance(layer, InputLayer):\n",
    "        return \"        X_input = X = Input({})\\n\".format(\n",
    "                layer.shape)\n",
    "        \n",
    "            \n",
    "    # if is Convolution\n",
    "    elif isinstance(layer, Convolution):\n",
    "        return \"        X = Conv2D({}, kernel_size={}, strides={}, activation='{}', padding={})(X)\\n\".format(\n",
    "                layer.nb_filter,\n",
    "                layer.kernel,\n",
    "                layer.stride,\n",
    "                layer.fct_activation,\n",
    "                layer.padding)\n",
    "            \n",
    "    # if is Pooling\n",
    "    elif isinstance(layer, Pooling):\n",
    "        if(layer.op == \"avg\"): # avg Pooling \n",
    "            return \"        X = AveragePooling2D(pool_size={}, strides={}, padding={})(X)\\n\".format(\n",
    "                    layer.kernel,\n",
    "                    layer.stride,\n",
    "                    layer.padding)\n",
    "                \n",
    "        else : # Max Pooling\n",
    "            return  \"        X = MaxPooling2D(pool_size={}, strides={}, padding={})(X)\\n\".format(\n",
    "                    layer.kernel,\n",
    "                    layer.stride,\n",
    "                    layer.padding)\n",
    "    elif isinstance(layer, IdBlock):\n",
    "        return \"        X = id_block(X, {}, {})\\n\".format(layer.kernel, layer.nb_filter)\n",
    "    elif isinstance(layer, ConvBlock):\n",
    "        return \"        X = conv_block(X, {}, {}, {})\\n\".format(layer.kernel, layer.nb_filter, layer.stride)\n",
    "    \n",
    "    # Not possible\n",
    "    else : print(\"Not Possible\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def end_CNN(layer):\n",
    "    # if is Dense (aka Fully connected layer)\n",
    "    if isinstance(layer, Dense):\n",
    "        return \"    head_model = Dense({}, activation='{}')(head_model)\\n\".format(\n",
    "                        layer.nb_neurones, \n",
    "                        layer.fct_activation)\n",
    "    # if is flatten\n",
    "    elif isinstance(layer, Flatten):\n",
    "        return \"    head_model = Flatten()(head_model)\\n\"\n",
    "    # Not possible\n",
    "    else : print(\"Not Possible\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def create_CNN(archi):\n",
    "    \n",
    "    i = 0\n",
    "    while not (isinstance(archi[i], Flatten)):\n",
    "        i+=1\n",
    "\n",
    "    end_archi = archi[i:]\n",
    "    \n",
    "    str_model_cnn = \"def ResNet():\\n\"\n",
    "    \n",
    "    \n",
    "    for layer in archi[:i]:\n",
    "        str_model_cnn += write_layer(layer)\n",
    "    \n",
    "        \n",
    "    str_model_cnn += \"        model = Model(inputs=X_input, outputs=X, name='ResNet18')\\n\"\n",
    "    str_model_cnn += \"        return model\\n\\n\"\n",
    "    str_model_cnn += \"    Input = ResNet()\\n\"\n",
    "    str_model_cnn += \"    head_model = Input.output\\n\"\n",
    "    for end_layer in end_archi:\n",
    "        str_model_cnn += end_CNN(end_layer)\n",
    "\n",
    "    str_model_cnn += \"    model = Model(inputs=Input.input, outputs=head_model)\\n\"\n",
    "    return str_model_cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_py_file(json_file):\n",
    "    s = json_file.split(\".\")\n",
    "    file_name = s[0].split(\"/\")[1]\n",
    "    architecture = readJsonFile(json_file)\n",
    "    \n",
    "    CNN_tex(file_name, architecture)\n",
    "    \n",
    "    # python directory\n",
    "    py_dir = \"architecture_py/\"\n",
    "    \n",
    "    # log directory\n",
    "    log_dir = \"../architecture_log/\"\n",
    "    \n",
    "    # png directory\n",
    "    png_dir = \"../architecture_img/\"\n",
    "    \n",
    "    # reset file\n",
    "    file_py = open(py_dir + file_name + \".py\", \"w\")\n",
    "    file_py.close()\n",
    "    \n",
    "    file_py = open(py_dir + file_name + \".py\", \"a\") # Open file in writting (a --> append)\n",
    "    \n",
    "    # write import\n",
    "    file_py.write(writeImport())\n",
    "    \n",
    "    # write train/test data \n",
    "    file_py.write(writeMnistDataset())\n",
    "    \n",
    "    file_py.write(\"\"\"\n",
    "\n",
    "# init training time\n",
    "training_time = 0\n",
    "# init result test/train\n",
    "test_result_loss = \"\"\n",
    "test_result_acc = \"\"\n",
    "\n",
    "train_result_loss = \"\"\n",
    "train_result_acc = \"\"\n",
    "\n",
    "nb_layers = \"not build\"\n",
    "\n",
    "\"\"\")\n",
    "    \n",
    "    # try\n",
    "    file_py.write(\"\"\"try:\n",
    "    \"\"\")\n",
    "    \n",
    "    # write function for id block\n",
    "    file_py.write(write_identity_block())\n",
    "    \n",
    "    # write function for conv block\n",
    "    file_py.write(write_conv_block())\n",
    "    \n",
    "    # write architecture model\n",
    "    file_py.write(create_CNN(architecture))\n",
    "    \n",
    "    \n",
    "    # write : create png of the model\n",
    "    file_py.write(\"    plot_model(model, show_shapes=True, to_file=\\\"%s\\\")\\n\" % (png_dir+file_name+\".png\"))\n",
    "    \n",
    "    # write compiler\n",
    "    file_py.write(\"\"\"    model.compile(optimizer='adam', loss=keras.losses.sparse_categorical_crossentropy, metrics=['accuracy'])\\n\\n\"\"\")\n",
    "    \n",
    "    # write way for time computation\n",
    "    file_py.write(\"\"\"    start = time()\\n\"\"\")\n",
    "    \n",
    "    # write model training\n",
    "    file_py.write(\"\"\"    model.fit(train_x, train_y, epochs=5, validation_data=(val_x, val_y))\\n\"\"\")\n",
    "    \n",
    "    # write time computation\n",
    "    file_py.write(\"\"\"    training_time = time()-start\\n\"\"\")\n",
    "    \n",
    "    # write model evaluation\n",
    "    file_py.write(\"\"\"    print(model.evaluate(test_x, test_y))\\n\"\"\")\n",
    "    \n",
    "    \n",
    "    # all is great\n",
    "    log_file = log_dir + file_name +\".log\"\n",
    "    file_py.write(\"\"\"\n",
    "    log_file = open(\\\"\"\"\" + log_file + \"\"\"\\\" , \"w\")\n",
    "    \n",
    "    # save test result\n",
    "    log_file.write('test result : ' + str(model.evaluate(test_x, test_y)))\n",
    "    test_result_loss = model.evaluate(test_x, test_y)[0]\n",
    "    test_result_acc = model.evaluate(test_x, test_y)[1]\n",
    "    \n",
    "    # save train result\n",
    "    log_file.write('train result : ' + str(model.evaluate(test_x, test_y)))\n",
    "    train_result_loss = model.evaluate(train_x, train_y)[0]\n",
    "    train_result_acc = model.evaluate(train_x, train_y)[1]\n",
    "    \n",
    "    print('OK: file \"\"\" + log_file +\"\"\" has been create')\n",
    "    \n",
    "    nb_layers = len(model.layers)\n",
    "    log_file.close()\n",
    "\"\"\")\n",
    "    \n",
    "    # something go wrong \n",
    "    error_file = log_dir + file_name + \"_error.log\"\n",
    "    file_py.write(\"\"\"except:\n",
    "    print('error: file \"\"\" + error_file +\"\"\" has been create')\n",
    "    error_file = open(\\\"\"\"\" + error_file + \"\"\"\\\" , \"w\")\n",
    "    traceback.print_exc(file=error_file)\n",
    "    result_loss = \"Error\"\n",
    "    result_acc = \"Error\"\n",
    "    error_file.close()\n",
    "\"\"\")\n",
    "    \n",
    "    file_py.write(\"\"\"finally:\n",
    "    file = open('../architecture_results_resnet.csv', 'a', newline ='')\n",
    "    with file: \n",
    "\n",
    "        # identifying header   \n",
    "        header = ['file_name', 'training_time(s)', 'test_result_loss', 'test_result_acc', 'train_result_acc', 'train_result_loss', 'nb_layers'] \n",
    "        writer = csv.DictWriter(file, fieldnames = header) \n",
    "      \n",
    "        # writing data row-wise into the csv file \n",
    "        # writer.writeheader() \n",
    "        writer.writerow({'file_name' : '\"\"\"+ file_name + \"\"\"',  \n",
    "                         'training_time(s)': training_time,  \n",
    "                         'test_result_loss': test_result_loss,\n",
    "                         'test_result_acc': test_result_acc,\n",
    "                         'train_result_acc': train_result_acc,\n",
    "                         'train_result_loss': train_result_loss,\n",
    "                         'nb_layers': nb_layers}) \n",
    "        print('add line into architecture_results.csv')\n",
    "    file.close()\n",
    "    \"\"\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # close\n",
    "    file_py.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"architecture_json/\"\n",
    "archi = readJsonFile(directory+\"archi_resnet_test.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<__main__.InputLayer at 0x12f41f17208>,\n",
       " <__main__.Convolution at 0x12f41f17048>,\n",
       " <__main__.Pooling at 0x12f41f17d48>,\n",
       " <__main__.ConvBlock at 0x12f41f17688>,\n",
       " <__main__.IdBlock at 0x12f41f17508>,\n",
       " <__main__.ConvBlock at 0x12f41f17748>,\n",
       " <__main__.IdBlock at 0x12f41f17bc8>,\n",
       " <__main__.ConvBlock at 0x12f41f17788>,\n",
       " <__main__.Flatten at 0x12f41f17308>,\n",
       " <__main__.Dense at 0x12f41f173c8>,\n",
       " <__main__.Dense at 0x12f41f17f48>]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "archi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Py file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture_json/archi_resnet_1.json\n",
      "\n",
      "architecture_json/archi_resnet_2.json\n",
      "\n",
      "architecture_json/archi_resnet_3.json\n",
      "\n",
      "architecture_json/archi_resnet_4.json\n",
      "\n",
      "architecture_json/archi_resnet_5.json\n",
      "\n",
      "architecture_json/archi_resnet_6.json\n",
      "\n",
      "architecture_json/archi_resnet_7.json\n",
      "\n",
      "architecture_json/archi_resnet_8.json\n",
      "\n",
      "architecture_json/archi_resnet_9.json\n",
      "\n",
      "architecture_json/archi_resnet_10.json\n",
      "\n"
     ]
    }
   ],
   "source": [
    "directory = \"architecture_json/\"\n",
    "for i in range(1,11):\n",
    "    file_name = \"archi_resnet_\"+str(i)+\".json\"\n",
    "    print(directory+file_name)\n",
    "    create_py_file(directory+file_name)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
