{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "from enum import Enum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolution\n",
    "class Convolution:\n",
    "    def __init__(self, kernel, padding, stride, nb_filter, fct_activation):\n",
    "        self.kernel = kernel\n",
    "        self.padding = padding\n",
    "        self.stride = stride\n",
    "        self.nb_filter = nb_filter\n",
    "        self.fct_activation = fct_activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input\n",
    "class InputLayer:\n",
    "    def __init__(self, shape):\n",
    "        self.shape = shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pooling Avg/Max\n",
    "class Pooling:\n",
    "    def __init__(self, op, kernel=2, padding=\"valid\", stride=None):\n",
    "        self.op = op # is the operation wanted (avg/max)\n",
    "        self.kernel = kernel\n",
    "        self.padding = padding\n",
    "        self.stride = stride"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class Flatten\n",
    "class Flatten:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dense --> Fully connected layer\n",
    "class Dense:\n",
    "    def __init__(self, nb_neurones, fct_activation):\n",
    "        self.nb_neurones = nb_neurones\n",
    "        self.fct_activation = fct_activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identity block \n",
    "class IdBlock:\n",
    "    def __init__(self, kernel, padding, nb_filter, fct_activation):\n",
    "        self.kernel = kernel\n",
    "        self.padding = padding\n",
    "        self.nb_filter = nb_filter\n",
    "        self.stride = 1\n",
    "        self.fct_activation = fct_activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convolution block\n",
    "class ConvBlock:\n",
    "    def __init__(self, kernel, padding, nb_filter, stride, fct_activation):\n",
    "        self.kernel = kernel\n",
    "        self.nb_filter = nb_filter\n",
    "        self.padding = padding\n",
    "        self.stride = stride\n",
    "        self.fct_activation = fct_activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enum type archi\n",
    "class TypeArchi(Enum):\n",
    "    ALL = 0\n",
    "    LENET = 1\n",
    "    RESNET = 2\n",
    "    DENSENET = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuration Archi\n",
    "class ConfArchi:\n",
    "    def __init__(self, type_archi, epsilon, dropout_rate, compress_factor):\n",
    "        self.type_archi = type_archi\n",
    "        self.epsilon = epsilon\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.compress_factor = compress_factor\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DenseNet Convolution Block \n",
    "class DenseNetBlock:\n",
    "    def __init__(self, kernel, padding, nb_filter, stride, nb_layer, fct_activation, op, nb_block_densenet):\n",
    "        self.kernel = kernel\n",
    "        self.nb_filter = nb_filter\n",
    "        self.padding = padding\n",
    "        self.stride = stride\n",
    "        self.nb_layer = nb_layer\n",
    "        self.fct_activation = fct_activation\n",
    "        self.op = op\n",
    "        self.nb_block_densenet = nb_block_densenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global pooling \n",
    "class GlobalPooling:\n",
    "    def __init__(self, op):\n",
    "        self.op = op    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read json file \n",
    "# return list of layer + graph\n",
    "def readJsonFile(json_file):\n",
    "    with open(json_file) as archi_json:\n",
    "        file = json.load(archi_json)\n",
    "    \n",
    "    # instantiate class\n",
    "    list_layer = []\n",
    "    conf_archi = \"\"\n",
    "    for layer in file:\n",
    "        parameters = layer[\"parameters\"] # get parameters\n",
    "        # if is Input class\n",
    "        if(layer[\"class\"] == \"InputLayer\"):\n",
    "            # instantiate class and add into the list\n",
    "            list_layer.append(InputLayer(parameters[\"shape\"]))\n",
    "\n",
    "        \n",
    "        # if is Pooling class\n",
    "        elif(layer[\"class\"] == \"Pooling\"):\n",
    "               \n",
    "            list_layer.append(Pooling(parameters[\"op\"],\n",
    "                                      parameters[\"kernel\"],\n",
    "                                      parameters[\"padding\"],\n",
    "                                      parameters[\"stride\"]))\n",
    "            \n",
    "        # if is Convolution class\n",
    "        elif(layer[\"class\"] == \"Convolution\"):\n",
    "                \n",
    "                \n",
    "            list_layer.append(Convolution(parameters[\"kernel\"],\n",
    "                                          parameters[\"padding\"],\n",
    "                                          parameters[\"stride\"],\n",
    "                                          parameters[\"nb_filter\"],\n",
    "                                          parameters[\"fct_activation\"]))\n",
    "            \n",
    "        # if is Flatten class\n",
    "        elif(layer[\"class\"] == \"Flatten\"):\n",
    "            list_layer.append(Flatten())\n",
    "        \n",
    "        # if is Dense class\n",
    "        elif(layer[\"class\"] == \"Dense\"):\n",
    "            list_layer.append(Dense(parameters[\"nb_neurones\"],\n",
    "                                    parameters[\"fct_activation\"]))\n",
    "        \n",
    "        elif(layer[\"class\"] == \"IdBlock\"):\n",
    "            list_layer.append(IdBlock(parameters[\"kernel\"],\n",
    "                                    parameters[\"padding\"],\n",
    "                                    parameters[\"nb_filter\"],\n",
    "                                    parameters[\"fct_activation\"]))\n",
    "            \n",
    "        elif(layer[\"class\"] == \"ConvBlock\"):\n",
    "            list_layer.append(ConvBlock(parameters[\"kernel\"],\n",
    "                                        parameters[\"padding\"],\n",
    "                                        parameters[\"nb_filter\"],\n",
    "                                        parameters[\"stride\"],\n",
    "                                        parameters[\"fct_activation\"]))\n",
    "        elif(layer[\"class\"] == \"DenseNetBlock\"):\n",
    "            list_layer.append(DenseNetBlock(parameters[\"kernel\"], \n",
    "                                            parameters[\"padding\"],\n",
    "                                            parameters[\"nb_filter\"], \n",
    "                                            parameters[\"stride\"], \n",
    "                                            parameters[\"nb_layer\"],\n",
    "                                            parameters[\"fct_activation\"],\n",
    "                                            parameters[\"op\"],\n",
    "                                            parameters[\"nb_block_densenet\"]))\n",
    "        elif(layer[\"class\"] == \"GlobalPooling\"):\n",
    "            list_layer.append(GlobalPooling(parameters[\"op\"]))\n",
    "        \n",
    "        elif(layer[\"class\"] == \"ConfArchi\"):\n",
    "            conf_archi = ConfArchi(parameters[\"type_archi\"], \n",
    "                                   parameters[\"epsilon\"], \n",
    "                                   parameters[\"dropout_rate\"], \n",
    "                                   parameters[\"compress_factor\"]\n",
    "                                  )\n",
    "\n",
    "            \n",
    "        else : print(\"Error\")\n",
    "    return list_layer, conf_archi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return import of the py file\n",
    "def writeImport():\n",
    "    return \"\"\"\n",
    "import numpy as np\n",
    "import os\n",
    "from keras import backend as K\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import Sequential, Model,load_model\n",
    "from tensorflow.keras.layers import Input, Add, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPooling2D, GlobalMaxPooling2D, GlobalAveragePooling2D, MaxPool2D, Concatenate, Dropout\n",
    "from tensorflow.keras.initializers import glorot_uniform\n",
    "from tensorflow.keras.utils import plot_model\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "import traceback\n",
    "import csv\n",
    "from time import time\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeGlobalVariable(config):\n",
    "    str_global = \"\\ntype_archi = '\" + str(TypeArchi(config.type_archi).name) + \"'\\n\"\n",
    "    str_global += \"epsilon = \" + str(config.epsilon) + \"\\n\"\n",
    "    str_global += \"dropout_rate = \"+ str( config.dropout_rate) + \"\\n\"\n",
    "    str_global += \"axis = 3\\n\"\n",
    "    str_global += \"compress_factor = \"+ str(config.compress_factor) + \"\\n\\n\"\n",
    "    \n",
    "    return str_global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write mnist data set\n",
    "def writeMnistDataset():\n",
    "    return \"\"\"(train_x, train_y), (test_x, test_y) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# normaliser les pixel 0-255 -> 0-1\n",
    "train_x = train_x / 255.0\n",
    "test_x = test_x / 255.0\n",
    "\n",
    "train_x = tf.expand_dims(train_x, 3)\n",
    "test_x = tf.expand_dims(test_x, 3)\n",
    "\n",
    "val_x = train_x[:5000]\n",
    "val_y = train_y[:5000]\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write cifar data set\n",
    "def writecifar10Dataset():\n",
    "    return \"\"\"\n",
    "# load dataset\n",
    "(train_x, train_y), (test_x, test_y) = keras.datasets.cifar10.load_data()\n",
    "\n",
    "# normalize to range 0-1\n",
    "train_x = train_x / 255.0\n",
    "test_x = test_x / 255.0\n",
    "\n",
    "val_x = train_x[:5000]\n",
    "val_y = train_y[:5000]\n",
    "    \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write function for identity block\n",
    "def write_identity_block():\n",
    "    return \"\"\"\n",
    "def id_block(X, f, filters, activation):\n",
    "\n",
    "    X_shortcut = X\n",
    "\n",
    "    X = Conv2D(filters=filters, kernel_size=(1, 1), strides=(1, 1), padding='same', kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    if epsilon != 0:\n",
    "        X = BatchNormalization(epsilon = epsilon, axis=axis)(X)\n",
    "    X = Activation(activation)(X)\n",
    "\n",
    "\n",
    "    X = Conv2D(filters=filters, kernel_size=(f, f), strides=(1, 1), padding='same', kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    if epsilon != 0:\n",
    "        X = BatchNormalization(epsilon = epsilon, axis=axis)(X)\n",
    "\n",
    "    X = Add()([X, X_shortcut])# SKIP Connection\n",
    "    X = Activation(activation)(X)\n",
    "\n",
    "    return X\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_conv_block():\n",
    "    return\"\"\"\n",
    "def conv_block(X, f, filters, activation, s=2):\n",
    "\n",
    "    X_shortcut = X\n",
    "\n",
    "    X = Conv2D(filters=filters, kernel_size=(1, 1), strides=(s, s), padding='valid', kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    if epsilon != 0:\n",
    "        X = BatchNormalization(epsilon = epsilon, axis=axis)(X)\n",
    "    X = Activation(activation)(X)\n",
    "\n",
    "    X = Conv2D(filters=filters, kernel_size=(f, f), strides=(1, 1), padding='same', kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    if epsilon != 0:\n",
    "        X = BatchNormalization(epsilon = epsilon, axis=axis)(X)\n",
    "\n",
    "    X_shortcut = Conv2D(filters=filters, kernel_size=(1, 1), strides=(s, s), padding='valid', kernel_initializer=glorot_uniform(seed=0))(X_shortcut)\n",
    "    if epsilon != 0:\n",
    "        X_shortcut = BatchNormalization(epsilon = epsilon, axis=axis)(X_shortcut)\n",
    "\n",
    "\n",
    "    X = Add()([X, X_shortcut])\n",
    "    X = Activation(activation)(X)\n",
    "\n",
    "    return X\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_dense_block():\n",
    "    return\"\"\"\n",
    "def denseBlock(X, f, nb_filter, nb_layer, padding, activation):\n",
    "    x_input = X    \n",
    "    for _ in range(0,nb_layer):\n",
    "        if epsilon != 0:\n",
    "            X = BatchNormalization(epsilon = epsilon, axis=axis)(X)\n",
    "        X = Activation(activation)(X)\n",
    "        X = Conv2D(filters=nb_filter, kernel_size=(f, f), strides=(1, 1), padding=padding)(X)\n",
    "        if dropout_rate != 0:\n",
    "            X = Dropout(dropout_rate)(X)\n",
    "    X = Concatenate()([X, x_input])\n",
    "    return X\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_transition_block():\n",
    "    return\"\"\"\n",
    "def transition_block(X, f, nb_filter, padding, activation, op, stride):\n",
    "    if epsilon != 0:\n",
    "            X = BatchNormalization(epsilon = epsilon, axis=axis)(X)\n",
    "    X = Activation(activation)(X)\n",
    "    X = Conv2D(filters=nb_filter, kernel_size=(f, f), strides=(1, 1), padding=padding)(X)\n",
    "    if dropout_rate != 0:\n",
    "        X = Dropout(dropout_rate)(X)\n",
    "\n",
    "    if (op == 'avg'):\n",
    "        X = AveragePooling2D(pool_size = f, strides=stride, padding=padding)(X)\n",
    "    else :\n",
    "        X = MaxPooling2D(pool_size=f, strides=stride, padding=padding)(X)\n",
    "\n",
    "    return X\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_init_value():\n",
    "    return \"\"\"\n",
    "\n",
    "# init training time\n",
    "training_time = 0\n",
    "# init result test/train\n",
    "test_result_loss = \"\"\n",
    "test_result_acc = \"\"\n",
    "\n",
    "train_result_loss = \"\"\n",
    "train_result_acc = \"\"\n",
    "\n",
    "nb_layers = \"not build\"\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return string of the wanted layer\n",
    "def write_layer(layer):\n",
    "    \n",
    "    # if is Intput\n",
    "    if isinstance(layer, InputLayer):\n",
    "        return \"        X_input = X = Input({})\\n\".format(\n",
    "                layer.shape)\n",
    "        \n",
    "            \n",
    "    # if is Convolution\n",
    "    elif isinstance(layer, Convolution):\n",
    "        return \"        X = Conv2D({}, kernel_size={}, strides={}, activation='{}', padding='{}')(X)\\n\".format(\n",
    "                layer.nb_filter,\n",
    "                layer.kernel,\n",
    "                layer.stride,\n",
    "                layer.fct_activation,\n",
    "                layer.padding)\n",
    "            \n",
    "    # if is Pooling\n",
    "    elif isinstance(layer, Pooling):\n",
    "        if(layer.op == \"avg\"): # avg Pooling \n",
    "            return \"        X = AveragePooling2D(pool_size={}, strides={}, padding='{}')(X)\\n\".format(\n",
    "                    layer.kernel,\n",
    "                    layer.stride,\n",
    "                    layer.padding)\n",
    "                \n",
    "        else : # Max Pooling\n",
    "            return  \"        X = MaxPooling2D(pool_size={}, strides={}, padding='{}')(X)\\n\".format(\n",
    "                    layer.kernel,\n",
    "                    layer.stride,\n",
    "                    layer.padding)\n",
    "    elif isinstance(layer, IdBlock):\n",
    "        return \"        X = id_block(X, {}, {}, '{}')\\n\".format(layer.kernel, layer.nb_filter, layer.fct_activation)\n",
    "    elif isinstance(layer, ConvBlock):\n",
    "        return \"        X = conv_block(X, {}, {}, '{}', {})\\n\".format(layer.kernel, layer.nb_filter, layer.fct_activation, layer.stride)\n",
    "    elif isinstance(layer, DenseNetBlock):\n",
    "        str_dense_block = \"\"\n",
    "        for nb in range(0,layer.nb_block_densenet):\n",
    "            str_dense_block += \"        X = denseBlock(X, {}, {}, {}, '{}', '{}')\\n\".format(\n",
    "                layer.kernel, \n",
    "                layer.nb_filter,\n",
    "                layer.nb_layer,\n",
    "                layer.padding,\n",
    "                layer.fct_activation\n",
    "            )\n",
    "        \n",
    "        str_dense_block += \"        X = transition_block(X, {}, {}, '{}', '{}', '{}', {})\\n\".format(\n",
    "            layer.kernel, \n",
    "            layer.nb_filter, \n",
    "            layer.padding,\n",
    "            layer.fct_activation,\n",
    "            layer.op,\n",
    "            layer.stride\n",
    "        )\n",
    "        return str_dense_block\n",
    "    elif isinstance(layer, GlobalPooling):\n",
    "        if(layer.op == \"avg\"):\n",
    "            return \"        X = GlobalAveragePooling2D()(X)\\n\"\n",
    "        else:\n",
    "            return \"        X = GlobalMaxPooling2D()(X)\\n\"\n",
    "    \n",
    "    elif isinstance(layer, Flatten):\n",
    "        return \"        X = Flatten()(X)\\n\"\n",
    "    elif isinstance(layer, Dense):\n",
    "        return \"        X = Dense({}, activation='{}')(X)\\n\".format(\n",
    "                        layer.nb_neurones, \n",
    "                        layer.fct_activation)\n",
    "    # Not possible\n",
    "    else : print(\"Not Possible\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_CNN(archi):\n",
    "    \n",
    "    str_model_cnn = \"def getModel():\\n\"\n",
    "    \n",
    "    for l in archi:\n",
    "        str_model_cnn += write_layer(l)\n",
    "    \n",
    "    str_model_cnn += \"        model = Model(inputs=X_input, outputs=X)\\n\"\n",
    "    str_model_cnn += \"        return model\\n\\n\"\n",
    "    str_model_cnn += \"    model = getModel()\\n\"\n",
    "    return str_model_cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_py_file(json_file):\n",
    "    s = json_file.split(\".\")\n",
    "    file_name = s[0].split(\"/\")[1]\n",
    "    architecture, config = readJsonFile(json_file)\n",
    "    \n",
    "    type_archi = TypeArchi(config.type_archi)\n",
    "    \n",
    "    print(type_archi)\n",
    "    \n",
    "    # python directory\n",
    "    py_dir = \"architecture_py/\"\n",
    "    \n",
    "    # log directory\n",
    "    log_dir = \"../architecture_log/\"\n",
    "    \n",
    "    # png directory\n",
    "    png_dir = \"../architecture_img/\"\n",
    "    \n",
    "    # reset file\n",
    "    file_py = open(py_dir + file_name + \".py\", \"w\")\n",
    "    file_py.close()\n",
    "    \n",
    "    file_py = open(py_dir + file_name + \".py\", \"a\") # Open file in writting (a --> append)\n",
    "    \n",
    "    # write import\n",
    "    file_py.write(writeImport())\n",
    "    \n",
    "    #write global variable\n",
    "    file_py.write(writeGlobalVariable(config))\n",
    "    \n",
    "    # write train/test data \n",
    "    file_py.write(writecifar10Dataset())\n",
    "    \n",
    "    # write init value \n",
    "    file_py.write(write_init_value())\n",
    "    \n",
    "    # add function\n",
    "    if (type_archi == TypeArchi.ALL):\n",
    "        file_py.write(write_identity_block())\n",
    "        file_py.write(write_conv_block())\n",
    "        file_py.write(write_dense_block())\n",
    "        file_py.write(write_transition_block())\n",
    "    \n",
    "    elif (type_archi == TypeArchi.DENSENET):\n",
    "        file_py.write(write_dense_block())\n",
    "        file_py.write(write_transition_block())\n",
    "    \n",
    "    elif (type_archi == TypeArchi.RESNET):\n",
    "        file_py.write(write_identity_block())\n",
    "        file_py.write(write_conv_block())\n",
    "        \n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    # try\n",
    "    file_py.write(\"\"\"\n",
    "try:\n",
    "    \"\"\")\n",
    "    \n",
    "    # write architecture model\n",
    "    file_py.write(create_CNN(architecture,))\n",
    "        \n",
    "    # write : create png of the model\n",
    "    file_py.write(\"    plot_model(model, show_shapes=True, to_file=\\\"%s\\\")\\n\" % (png_dir+file_name+\".png\"))\n",
    "    \n",
    "    # write compiler\n",
    "    file_py.write(\"\"\"    model.compile(optimizer='adam', loss=keras.losses.sparse_categorical_crossentropy, metrics=['accuracy'])\\n\\n\"\"\")\n",
    "    \n",
    "    # write way for time computation\n",
    "    file_py.write(\"\"\"    start = time()\\n\"\"\")\n",
    "    \n",
    "    # write callbacks function\n",
    "    file_py.write(\"\"\"    es = tf.keras.callbacks.EarlyStopping(monitor='loss', verbose=1)\n",
    "    list_cb = [es]\\n\"\"\")\n",
    "    \n",
    "    # write model training\n",
    "    file_py.write(\"\"\"    history = model.fit(train_x, train_y, epochs=50, validation_split=0.1, callbacks=list_cb)\\n\"\"\")\n",
    "    \n",
    "    # write time computation\n",
    "    file_py.write(\"\"\"    training_time = time()-start\\n\"\"\")\n",
    "    \n",
    "    # write model evaluation\n",
    "    file_py.write(\"\"\"    print(model.evaluate(test_x, test_y))\\n\"\"\")\n",
    "    \n",
    "    \n",
    "    # all is great\n",
    "    log_file = log_dir + file_name +\".log\"\n",
    "    file_py.write(\"\"\"\n",
    "    log_file = open(\\\"\"\"\" + log_file + \"\"\"\\\" , \"w\")\n",
    "    \n",
    "    # save test result\n",
    "    log_file.write('test result : ' + str(model.evaluate(test_x, test_y)))\n",
    "    test_result_loss = model.evaluate(test_x, test_y)[0]\n",
    "    test_result_acc = model.evaluate(test_x, test_y)[1]\n",
    "    \n",
    "    # save train result\n",
    "    log_file.write('train result : ' + str(model.evaluate(test_x, test_y)))\n",
    "    log_file.write('History train result : ' + str(history.history))\n",
    "    train_result_loss = model.evaluate(train_x, train_y)[0]\n",
    "    train_result_acc = model.evaluate(train_x, train_y)[1]\n",
    "    \n",
    "    print('OK: file \"\"\" + log_file +\"\"\" has been create')\n",
    "    \n",
    "    nb_layers = len(model.layers)\n",
    "    log_file.close()\n",
    "\"\"\")\n",
    "    \n",
    "    # something go wrong \n",
    "    error_file = log_dir + file_name + \"_error.log\"\n",
    "    file_py.write(\"\"\"except:\n",
    "    print('error: file \"\"\" + error_file +\"\"\" has been create')\n",
    "    error_file = open(\\\"\"\"\" + error_file + \"\"\"\\\" , \"w\")\n",
    "    traceback.print_exc(file=error_file)\n",
    "    result_loss = \"Error\"\n",
    "    result_acc = \"Error\"\n",
    "    error_file.close()\n",
    "\"\"\")\n",
    "    \n",
    "    file_py.write(\"\"\"finally:\n",
    "    file = open('../architecture_results_v3.csv', 'a', newline ='')\n",
    "    with file: \n",
    "\n",
    "        # identifying header   \n",
    "        header = ['file_name', 'training_time(s)', 'test_result_loss', 'test_result_acc', 'train_result_acc', 'train_result_loss', 'nb_layers', 'epochs', 'type_archi'] \n",
    "        writer = csv.DictWriter(file, fieldnames = header) \n",
    "      \n",
    "        # writing data row-wise into the csv file \n",
    "        # writer.writeheader() \n",
    "        writer.writerow({'file_name' : '\"\"\"+ file_name + \"\"\"',  \n",
    "                         'training_time(s)': training_time,  \n",
    "                         'test_result_loss': test_result_loss,\n",
    "                         'test_result_acc': test_result_acc,\n",
    "                         'train_result_acc': train_result_acc,\n",
    "                         'train_result_loss': train_result_loss,\n",
    "                         'nb_layers': nb_layers,\n",
    "                         'epochs' : len(history.history['loss']),\n",
    "                         'type_archi': type_archi}) \n",
    "        print('add line into architecture_results_v3.csv')\n",
    "    file.close()\n",
    "    \"\"\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    # close\n",
    "    file_py.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"architecture_json/\"\n",
    "archi, conf_archi = readJsonFile(directory+\"archi_V3_test.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<__main__.InputLayer at 0x297dd4edf88>,\n",
       " <__main__.Convolution at 0x297dd4ed7c8>,\n",
       " <__main__.DenseNetBlock at 0x297dd4edec8>,\n",
       " <__main__.DenseNetBlock at 0x297dd4edd88>,\n",
       " <__main__.DenseNetBlock at 0x297dd4ede08>,\n",
       " <__main__.GlobalPooling at 0x297dd4ed148>,\n",
       " <__main__.Dense at 0x297dd4edbc8>]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "archi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Py file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture_json/archi_v3_1.json\n",
      "TypeArchi.ALL\n",
      "\n",
      "architecture_json/archi_v3_2.json\n",
      "TypeArchi.ALL\n",
      "\n",
      "architecture_json/archi_v3_3.json\n",
      "TypeArchi.ALL\n",
      "\n",
      "architecture_json/archi_v3_4.json\n",
      "TypeArchi.ALL\n",
      "\n",
      "architecture_json/archi_v3_5.json\n",
      "TypeArchi.ALL\n",
      "\n",
      "architecture_json/archi_v3_6.json\n",
      "TypeArchi.ALL\n",
      "\n",
      "architecture_json/archi_v3_7.json\n",
      "TypeArchi.ALL\n",
      "\n",
      "architecture_json/archi_v3_8.json\n",
      "TypeArchi.ALL\n",
      "\n",
      "architecture_json/archi_v3_9.json\n",
      "TypeArchi.ALL\n",
      "\n",
      "architecture_json/archi_v3_10.json\n",
      "TypeArchi.ALL\n",
      "\n",
      "architecture_json/archi_v3_11.json\n",
      "TypeArchi.ALL\n",
      "\n",
      "architecture_json/archi_v3_12.json\n",
      "TypeArchi.ALL\n",
      "\n",
      "architecture_json/archi_v3_13.json\n",
      "TypeArchi.ALL\n",
      "\n",
      "architecture_json/archi_v3_14.json\n",
      "TypeArchi.ALL\n",
      "\n",
      "architecture_json/archi_v3_15.json\n",
      "TypeArchi.ALL\n",
      "\n",
      "architecture_json/archi_v3_16.json\n",
      "TypeArchi.ALL\n",
      "\n",
      "architecture_json/archi_v3_17.json\n",
      "TypeArchi.ALL\n",
      "\n",
      "architecture_json/archi_v3_18.json\n",
      "TypeArchi.ALL\n",
      "\n",
      "architecture_json/archi_v3_19.json\n",
      "TypeArchi.ALL\n",
      "\n",
      "architecture_json/archi_v3_20.json\n",
      "TypeArchi.ALL\n",
      "\n",
      "architecture_json/archi_v3_21.json\n",
      "TypeArchi.ALL\n",
      "\n",
      "architecture_json/archi_v3_22.json\n",
      "TypeArchi.ALL\n",
      "\n",
      "architecture_json/archi_v3_23.json\n",
      "TypeArchi.ALL\n",
      "\n",
      "architecture_json/archi_v3_24.json\n",
      "TypeArchi.ALL\n",
      "\n",
      "architecture_json/archi_v3_25.json\n",
      "TypeArchi.ALL\n",
      "\n",
      "architecture_json/archi_v3_26.json\n",
      "TypeArchi.LENET\n",
      "\n",
      "architecture_json/archi_v3_27.json\n",
      "TypeArchi.LENET\n",
      "\n",
      "architecture_json/archi_v3_28.json\n",
      "TypeArchi.LENET\n",
      "\n",
      "architecture_json/archi_v3_29.json\n",
      "TypeArchi.LENET\n",
      "\n",
      "architecture_json/archi_v3_30.json\n",
      "TypeArchi.LENET\n",
      "\n",
      "architecture_json/archi_v3_31.json\n",
      "TypeArchi.LENET\n",
      "\n",
      "architecture_json/archi_v3_32.json\n",
      "TypeArchi.LENET\n",
      "\n",
      "architecture_json/archi_v3_33.json\n",
      "TypeArchi.LENET\n",
      "\n",
      "architecture_json/archi_v3_34.json\n",
      "TypeArchi.LENET\n",
      "\n",
      "architecture_json/archi_v3_35.json\n",
      "TypeArchi.LENET\n",
      "\n",
      "architecture_json/archi_v3_36.json\n",
      "TypeArchi.LENET\n",
      "\n",
      "architecture_json/archi_v3_37.json\n",
      "TypeArchi.LENET\n",
      "\n",
      "architecture_json/archi_v3_38.json\n",
      "TypeArchi.LENET\n",
      "\n",
      "architecture_json/archi_v3_39.json\n",
      "TypeArchi.LENET\n",
      "\n",
      "architecture_json/archi_v3_40.json\n",
      "TypeArchi.LENET\n",
      "\n",
      "architecture_json/archi_v3_41.json\n",
      "TypeArchi.LENET\n",
      "\n",
      "architecture_json/archi_v3_42.json\n",
      "TypeArchi.LENET\n",
      "\n",
      "architecture_json/archi_v3_43.json\n",
      "TypeArchi.LENET\n",
      "\n",
      "architecture_json/archi_v3_44.json\n",
      "TypeArchi.LENET\n",
      "\n",
      "architecture_json/archi_v3_45.json\n",
      "TypeArchi.LENET\n",
      "\n",
      "architecture_json/archi_v3_46.json\n",
      "TypeArchi.LENET\n",
      "\n",
      "architecture_json/archi_v3_47.json\n",
      "TypeArchi.LENET\n",
      "\n",
      "architecture_json/archi_v3_48.json\n",
      "TypeArchi.LENET\n",
      "\n",
      "architecture_json/archi_v3_49.json\n",
      "TypeArchi.LENET\n",
      "\n",
      "architecture_json/archi_v3_50.json\n",
      "TypeArchi.LENET\n",
      "\n",
      "architecture_json/archi_v3_51.json\n",
      "TypeArchi.RESNET\n",
      "\n",
      "architecture_json/archi_v3_52.json\n",
      "TypeArchi.RESNET\n",
      "\n",
      "architecture_json/archi_v3_53.json\n",
      "TypeArchi.RESNET\n",
      "\n",
      "architecture_json/archi_v3_54.json\n",
      "TypeArchi.RESNET\n",
      "\n",
      "architecture_json/archi_v3_55.json\n",
      "TypeArchi.RESNET\n",
      "\n",
      "architecture_json/archi_v3_56.json\n",
      "TypeArchi.RESNET\n",
      "\n",
      "architecture_json/archi_v3_57.json\n",
      "TypeArchi.RESNET\n",
      "\n",
      "architecture_json/archi_v3_58.json\n",
      "TypeArchi.RESNET\n",
      "\n",
      "architecture_json/archi_v3_59.json\n",
      "TypeArchi.RESNET\n",
      "\n",
      "architecture_json/archi_v3_60.json\n",
      "TypeArchi.RESNET\n",
      "\n",
      "architecture_json/archi_v3_61.json\n",
      "TypeArchi.RESNET\n",
      "\n",
      "architecture_json/archi_v3_62.json\n",
      "TypeArchi.RESNET\n",
      "\n",
      "architecture_json/archi_v3_63.json\n",
      "TypeArchi.RESNET\n",
      "\n",
      "architecture_json/archi_v3_64.json\n",
      "TypeArchi.RESNET\n",
      "\n",
      "architecture_json/archi_v3_65.json\n",
      "TypeArchi.RESNET\n",
      "\n",
      "architecture_json/archi_v3_66.json\n",
      "TypeArchi.RESNET\n",
      "\n",
      "architecture_json/archi_v3_67.json\n",
      "TypeArchi.RESNET\n",
      "\n",
      "architecture_json/archi_v3_68.json\n",
      "TypeArchi.RESNET\n",
      "\n",
      "architecture_json/archi_v3_69.json\n",
      "TypeArchi.RESNET\n",
      "\n",
      "architecture_json/archi_v3_70.json\n",
      "TypeArchi.RESNET\n",
      "\n",
      "architecture_json/archi_v3_71.json\n",
      "TypeArchi.RESNET\n",
      "\n",
      "architecture_json/archi_v3_72.json\n",
      "TypeArchi.RESNET\n",
      "\n",
      "architecture_json/archi_v3_73.json\n",
      "TypeArchi.RESNET\n",
      "\n",
      "architecture_json/archi_v3_74.json\n",
      "TypeArchi.RESNET\n",
      "\n",
      "architecture_json/archi_v3_75.json\n",
      "TypeArchi.RESNET\n",
      "\n",
      "architecture_json/archi_v3_76.json\n",
      "TypeArchi.DENSENET\n",
      "\n",
      "architecture_json/archi_v3_77.json\n",
      "TypeArchi.DENSENET\n",
      "\n",
      "architecture_json/archi_v3_78.json\n",
      "TypeArchi.DENSENET\n",
      "\n",
      "architecture_json/archi_v3_79.json\n",
      "TypeArchi.DENSENET\n",
      "\n",
      "architecture_json/archi_v3_80.json\n",
      "TypeArchi.DENSENET\n",
      "\n",
      "architecture_json/archi_v3_81.json\n",
      "TypeArchi.DENSENET\n",
      "\n",
      "architecture_json/archi_v3_82.json\n",
      "TypeArchi.DENSENET\n",
      "\n",
      "architecture_json/archi_v3_83.json\n",
      "TypeArchi.DENSENET\n",
      "\n",
      "architecture_json/archi_v3_84.json\n",
      "TypeArchi.DENSENET\n",
      "\n",
      "architecture_json/archi_v3_85.json\n",
      "TypeArchi.DENSENET\n",
      "\n",
      "architecture_json/archi_v3_86.json\n",
      "TypeArchi.DENSENET\n",
      "\n",
      "architecture_json/archi_v3_87.json\n",
      "TypeArchi.DENSENET\n",
      "\n",
      "architecture_json/archi_v3_88.json\n",
      "TypeArchi.DENSENET\n",
      "\n",
      "architecture_json/archi_v3_89.json\n",
      "TypeArchi.DENSENET\n",
      "\n",
      "architecture_json/archi_v3_90.json\n",
      "TypeArchi.DENSENET\n",
      "\n",
      "architecture_json/archi_v3_91.json\n",
      "TypeArchi.DENSENET\n",
      "\n",
      "architecture_json/archi_v3_92.json\n",
      "TypeArchi.DENSENET\n",
      "\n",
      "architecture_json/archi_v3_93.json\n",
      "TypeArchi.DENSENET\n",
      "\n",
      "architecture_json/archi_v3_94.json\n",
      "TypeArchi.DENSENET\n",
      "\n",
      "architecture_json/archi_v3_95.json\n",
      "TypeArchi.DENSENET\n",
      "\n",
      "architecture_json/archi_v3_96.json\n",
      "TypeArchi.DENSENET\n",
      "\n",
      "architecture_json/archi_v3_97.json\n",
      "TypeArchi.DENSENET\n",
      "\n",
      "architecture_json/archi_v3_98.json\n",
      "TypeArchi.DENSENET\n",
      "\n",
      "architecture_json/archi_v3_99.json\n",
      "TypeArchi.DENSENET\n",
      "\n",
      "architecture_json/archi_v3_100.json\n",
      "TypeArchi.DENSENET\n",
      "\n"
     ]
    }
   ],
   "source": [
    "directory = \"architecture_json/\"\n",
    "for i in range(1,101):\n",
    "    file_name = \"archi_v3_%d.json\" % i\n",
    "    print(directory+file_name)\n",
    "    create_py_file(directory+file_name)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "254.2px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
