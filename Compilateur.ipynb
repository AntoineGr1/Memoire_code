{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolution\n",
    "class Convolution:\n",
    "    def __init__(self, kernel, padding, stride, nb_filter, fct_activation):\n",
    "        self.kernel = kernel\n",
    "        self.padding = padding\n",
    "        self.stride = stride\n",
    "        self.nb_filter = nb_filter\n",
    "        self.fct_activation = fct_activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input\n",
    "class InputLayer():\n",
    "    def __init__(self, shape):\n",
    "        self.shape = shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pooling Avg/Max\n",
    "class Pooling:\n",
    "    def __init__(self, op, kernel=2, padding=\"valid\", stride=None):\n",
    "        self.op = op # is the operation wanted (avg/max)\n",
    "        self.kernel = kernel\n",
    "        self.padding = padding\n",
    "        self.stride = stride"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class Flatten\n",
    "class Flatten:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dense --> Fully connected layer\n",
    "class Dense:\n",
    "    def __init__(self, nb_neurones, fct_activation):\n",
    "        self.nb_neurones = nb_neurones\n",
    "        self.fct_activation = fct_activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that create CNN with list of layer\n",
    "def create_CNN(list_layers):\n",
    "    str_model_cnn = \"model = keras.models.Sequential([\\n\"\n",
    "    # iterate List of layer\n",
    "    for layer in list_layers:\n",
    "        \n",
    "        # if is Intput\n",
    "        if isinstance(layer, InputLayer):\n",
    "            str_model_cnn += \"\\t\\tkeras.layers.Input({}),\\n\".format(\n",
    "                    layer.shape)\n",
    "            \n",
    "            \n",
    "        # if is Convolution\n",
    "        elif isinstance(layer, Convolution):\n",
    "            str_model_cnn += \"\\t\\tkeras.layers.Conv2D({}, kernel_size={}, strides={}, activation='{}', padding={}),\\n\".format(\n",
    "                    layer.nb_filter,\n",
    "                    layer.kernel,\n",
    "                    layer.stride,\n",
    "                    layer.fct_activation,\n",
    "                    layer.padding)\n",
    "            \n",
    "        # if is Pooling\n",
    "        elif isinstance(layer, Pooling):\n",
    "            if(layer.op == \"avg\"): # avg Pooling \n",
    "                str_model_cnn += \"\\t\\tkeras.layers.AveragePooling2D(pool_size={}, strides={}, padding={}),\\n\".format(\n",
    "                    layer.kernel,\n",
    "                    layer.stride,\n",
    "                    layer.padding)\n",
    "                \n",
    "            else : # Max Pooling\n",
    "                str_model_cnn += \"\\t\\tkeras.layers.MaxPooling2D(pool_size={}, strides={}, padding={}),\\n\".format(\n",
    "                    layer.kernel,\n",
    "                    layer.stride,\n",
    "                    layer.padding)\n",
    "                \n",
    "        # if is Dense (aka Fully connected layer)\n",
    "        elif isinstance(layer, Dense):\n",
    "            str_model_cnn += \"\\t\\tkeras.layers.Dense({}, activation='{}'),\\n\".format(\n",
    "                layer.nb_neurones, \n",
    "                layer.fct_activation)\n",
    "            \n",
    "        # if is flatten\n",
    "        elif isinstance(layer, Flatten):\n",
    "            str_model_cnn += \"\\t\\tkeras.layers.Flatten(),\\n\"\n",
    "            \n",
    "        # Not possible\n",
    "        else : print(\"Not Possible\")\n",
    "\n",
    "    # end model \n",
    "    str_model_cnn += \"\\n\\t])\\n\"\n",
    "    return str_model_cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read json file \n",
    "# return list of layer\n",
    "def getArchitectureFromJsonFile(json_file):\n",
    "    \n",
    "    # get architecture by json file\n",
    "    with open(json_file) as archi_json:\n",
    "        archi = json.load(archi_json)\n",
    "\n",
    "    # instantiate class\n",
    "    list_layer = []\n",
    "    for layer in archi:\n",
    "        parameters = layer[\"parameters\"] # get parameters\n",
    "        # if is Input class\n",
    "        if(layer[\"class\"] == \"InputLayer\"):\n",
    "            # instantiate class and add into the list\n",
    "            list_layer.append(InputLayer(parameters[\"shape\"]))\n",
    "\n",
    "        \n",
    "        # if is Pooling class\n",
    "        elif(layer[\"class\"] == \"Pooling\"):\n",
    "            try:\n",
    "                padding = int(parameters[\"padding\"])\n",
    "            except:\n",
    "                padding = \"'\" + parameters[\"padding\"] + \"'\"\n",
    "                \n",
    "            list_layer.append(Pooling(parameters[\"op\"],\n",
    "                                      parameters[\"kernel\"],\n",
    "                                      padding,\n",
    "                                      parameters[\"stride\"]))\n",
    "            \n",
    "        # if is Convolution class\n",
    "        elif(layer[\"class\"] == \"Convolution\"):\n",
    "            try:\n",
    "                padding = int(parameters[\"padding\"])\n",
    "            except:\n",
    "                padding = \"'\" + parameters[\"padding\"] + \"'\"\n",
    "                \n",
    "                \n",
    "            list_layer.append(Convolution(parameters[\"kernel\"],\n",
    "                                          padding,\n",
    "                                          parameters[\"stride\"],\n",
    "                                          parameters[\"nb_filter\"],\n",
    "                                          parameters[\"fct_activation\"]))\n",
    "            \n",
    "        # if is Flatten class\n",
    "        elif(layer[\"class\"] == \"Flatten\"):\n",
    "            list_layer.append(Flatten())\n",
    "        \n",
    "        # if is Dense class\n",
    "        elif(layer[\"class\"] == \"Dense\"):\n",
    "            list_layer.append(Dense(parameters[\"nb_neurones\"],\n",
    "                                    parameters[\"fct_activation\"]))\n",
    "\n",
    "        else : print(\"Error\")\n",
    "    return list_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_py_file(json_file):\n",
    "    s = json_file.split(\".\")\n",
    "    file_name = s[0].split(\"/\")[1]\n",
    "    architecture = getArchitectureFromJsonFile(json_file)\n",
    "    \n",
    "    # python directory\n",
    "    py_dir = \"architecture_py/\"\n",
    "    \n",
    "    # log directory\n",
    "    log_dir = \"../architecture_log/\"\n",
    "    \n",
    "    # png directory\n",
    "    png_dir = \"../architecture_img/\"\n",
    "    \n",
    "    # reset file\n",
    "    file_py = open(py_dir + file_name + \".py\", \"w\")\n",
    "    file_py.close()\n",
    "    \n",
    "    file_py = open(py_dir + file_name + \".py\", \"a\") # Open file in writting (a --> append)\n",
    "    \n",
    "    # write import\n",
    "    file_py.write(\"\"\"\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "from tensorflow.keras.utils import plot_model\n",
    "import sys\n",
    "import traceback\n",
    "import csv\n",
    "from time import time\n",
    "\"\"\")\n",
    "    \n",
    "    # write train/test data \n",
    "    file_py.write(\"\"\"(train_x, train_y), (test_x, test_y) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# normaliser les pixel 0-255 -> 0-1\n",
    "train_x = train_x / 255.0\n",
    "test_x = test_x / 255.0\n",
    "\n",
    "train_x = tf.expand_dims(train_x, 3)\n",
    "test_x = tf.expand_dims(test_x, 3)\n",
    "\n",
    "val_x = train_x[:5000]\n",
    "val_y = train_y[:5000]\n",
    "\n",
    "\n",
    "# init training time\n",
    "training_time = 0\n",
    "# init result\n",
    "result_loss = \"\"\n",
    "result_acc = \"\"\n",
    "\"\"\")\n",
    "    \n",
    "    # try\n",
    "    file_py.write(\"\"\"try:\n",
    "    \"\"\")\n",
    "    \n",
    "    # write architecture model\n",
    "    file_py.write(create_CNN(architecture))\n",
    "    \n",
    "    # write : create png of the model\n",
    "    file_py.write(\"    plot_model(model, show_shapes=True, to_file=\\\"%s\\\")\\n\" % (png_dir+file_name+\".png\"))\n",
    "    \n",
    "    # write compiler\n",
    "    file_py.write(\"\"\"    model.compile(optimizer='adam', loss=keras.losses.sparse_categorical_crossentropy, metrics=['accuracy'])\\n\"\"\")\n",
    "    \n",
    "    # write way for time computation\n",
    "    file_py.write(\"\"\"    start = time()\\n\"\"\")\n",
    "    \n",
    "    # write model training\n",
    "    file_py.write(\"\"\"    model.fit(train_x, train_y, epochs=5, validation_data=(val_x, val_y))\\n\"\"\")\n",
    "    \n",
    "    # write time computation\n",
    "    file_py.write(\"\"\"    training_time = time()-start\\n\"\"\")\n",
    "    \n",
    "    # write model evaluation\n",
    "    file_py.write(\"\"\"    print(model.evaluate(test_x, test_y))\\n\"\"\")\n",
    "    \n",
    "    # all is great\n",
    "    log_file = log_dir + file_name +\".log\"\n",
    "    file_py.write(\"\"\"\n",
    "    print('OK: file \"\"\" + log_file +\"\"\" has been create')\n",
    "    log_file = open(\\\"\"\"\" + log_file + \"\"\"\\\" , \"w\")\n",
    "    log_file.write(str(model.evaluate(test_x, test_y)))\n",
    "    result_loss = model.evaluate(test_x, test_y)[0]\n",
    "    result_acc = model.evaluate(test_x, test_y)[1]\n",
    "    log_file.close()\n",
    "\"\"\")\n",
    "    \n",
    "    # something go wrong \n",
    "    error_file = log_dir + file_name + \"_error.log\"\n",
    "    file_py.write(\"\"\"except:\n",
    "    print('error: file \"\"\" + error_file +\"\"\" has been create')\n",
    "    error_file = open(\\\"\"\"\" + error_file + \"\"\"\\\" , \"w\")\n",
    "    traceback.print_exc(file=error_file)\n",
    "    result_loss = \"Error\"\n",
    "    result_acc = \"Error\"\n",
    "    error_file.close()\n",
    "\"\"\")\n",
    "    \n",
    "    file_py.write(\"\"\"finally:\n",
    "    file = open('../architecture_results.csv', 'a', newline ='')\n",
    "    with file: \n",
    "\n",
    "        # identifying header   \n",
    "        header = ['file_name', 'training_time(s)', 'result_loss', 'result_acc'] \n",
    "        writer = csv.DictWriter(file, fieldnames = header) \n",
    "      \n",
    "        # writing data row-wise into the csv file \n",
    "        # writer.writeheader() \n",
    "        writer.writerow({'file_name' : '\"\"\"+ file_name + \"\"\"',  \n",
    "                         'training_time': training_time,  \n",
    "                         'result_loss': result_loss,\n",
    "                         'result_acc': result_acc}) \n",
    "        print('add line into architecture_results.csv')\n",
    "    \"\"\")\n",
    "    \n",
    "    # close\n",
    "    file_py.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creation CNN valide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "create_py_file(\"architecture_json/architecture_valid.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creation CNN non valide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_py_file(\"architecture_json/architecture_invalid.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create CNN json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture_json/archi_random_1_v.json\n",
      "\n",
      "architecture_json/archi_random_2_v.json\n",
      "\n",
      "architecture_json/archi_random_3_v.json\n",
      "\n",
      "architecture_json/archi_random_4_v.json\n",
      "\n",
      "architecture_json/archi_random_5_v.json\n",
      "\n",
      "architecture_json/archi_random_6_v.json\n",
      "\n",
      "architecture_json/archi_random_7_v.json\n",
      "\n",
      "architecture_json/archi_random_8_v.json\n",
      "\n",
      "architecture_json/archi_random_9_v.json\n",
      "\n",
      "architecture_json/archi_random_10_v.json\n",
      "\n"
     ]
    }
   ],
   "source": [
    "directory = \"architecture_json/\"\n",
    "for i in range(1,11):\n",
    "    file_name = \"archi_random_\"+str(i)+\"_v.json\"\n",
    "    print(directory+file_name)\n",
    "    create_py_file(directory+file_name)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "612px",
    "left": "62px",
    "top": "110.8px",
    "width": "278.2px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
