{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 524,
   "id": "a4470016",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jsonobject import *\n",
    "import numpy as np \n",
    "import random\n",
    "import math\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "id": "7488ee9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(2021)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649ac400",
   "metadata": {},
   "source": [
    "# Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "id": "5e885173",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolution\n",
    "class Convolution(JsonObject):\n",
    "    kernel = IntegerProperty()\n",
    "    padding = StringProperty()\n",
    "    stride = IntegerProperty()\n",
    "    nb_filter = IntegerProperty()\n",
    "    fct_activation = StringProperty()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "id": "289249b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input \n",
    "class InputLayer(JsonObject):\n",
    "    shape = ListProperty(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "id": "a60167d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pooling Avg/Max\n",
    "class Pooling(JsonObject):\n",
    "    op = StringProperty()\n",
    "    kernel = IntegerProperty(default=2)\n",
    "    padding = StringProperty(default=\"valid\")\n",
    "    stride = IntegerProperty(default=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "id": "f010355a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class Flatten\n",
    "class Flatten(JsonObject):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "id": "d8b40a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dense --> Fully connected layer\n",
    "class Dense(JsonObject):\n",
    "    nb_neurones =  IntegerProperty()\n",
    "    fct_activation = StringProperty()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "id": "63c0b3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batchnorm\n",
    "class BatchNormalisation(JsonObject):\n",
    "    epsilon = FloatProperty()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "id": "a5a61fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global pooling \n",
    "class GlobalPooling(JsonObject):\n",
    "    op = StringProperty()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "id": "c88668a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# classe for the different possible merge add/concatenate\n",
    "class Merge(JsonObject):\n",
    "    m_type = StringProperty()\n",
    "    m_function = StringProperty()\n",
    "    sub_SM_1 = ListProperty(JsonObject)\n",
    "    sub_SM_2 = ListProperty(JsonObject)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "id": "431d96d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class dropout\n",
    "class Dropout(JsonObject):\n",
    "    dropout_rate = FloatProperty()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "id": "aa7d7eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class that represente when the way of merge is empty\n",
    "class Empty(JsonObject):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16be5340",
   "metadata": {},
   "source": [
    "# Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "id": "b291447b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table of hyperparameter value\n",
    "kernel_value = [1, 2, 3, 4, 5, 6, 7]\n",
    "stride_value = [1, 2, 3, 4, 5, 6, 7]\n",
    "padding_value = [\"valid\", \"same\"]\n",
    "fct_activation_value = [\"tanh\", \"relu\", \"selu\"]\n",
    "\n",
    "epsilon_value = [1.1e-10, 1.001e-5, 0.001, 1.1e-5, 1.1e-7]\n",
    "dropout_value = [.2, .1, .4, .5, .8, .01, .001]\n",
    "\n",
    "nb_layer_value = [2,3,4,5,6,7]\n",
    "\n",
    "nb_class = 10 # nb_class\n",
    "op_value = ['avg',\"max\"]\n",
    "\n",
    "m_function_value = [\"add\", \"concatenate\"]\n",
    "\n",
    "classification = [Flatten, GlobalPooling]\n",
    "\n",
    "merge_type = [\"full\", \"empty\"]\n",
    "\n",
    "compress_factor=0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f63a06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3be668f9",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "id": "f4edaa80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def equals_sm1_sm2(ssm1, ssm2):\n",
    "    try:\n",
    "        i = 0\n",
    "        while (i<len(ssm1)):\n",
    "            if (ssm1[i] != ssm2[i]):\n",
    "                return False\n",
    "            i+=1\n",
    "        return True\n",
    "    except: return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "id": "848fafb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stop(size_archi, x):\n",
    "    prob = (math.log(2*size_archi)*x)-(2*x)\n",
    "    if ( prob < random.randrange(101)):\n",
    "        return True\n",
    "    else : return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "id": "b7b9c342",
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# Create Json file\n",
    "def create_json_file(archi, file_name):\n",
    "    directory = 'architecture_json/'\n",
    "    \n",
    "    # reset file\n",
    "    archi_file = open(directory+file_name, \"w\")\n",
    "    archi_file.close()\n",
    "    \n",
    "    # create file\n",
    "    \n",
    "    # open file\n",
    "    archi_file = open(directory+file_name, \"a\")\n",
    "    \n",
    "    archi_file.write(\"\"\"[\n",
    "    \"\"\")\n",
    "    \n",
    "    archi_size = len(archi)\n",
    "    i = 0\n",
    "    for l in archi:\n",
    "        if(isinstance(l, Merge)):\n",
    "            str_layer = \"\"\"{\n",
    "        'class':'\"\"\"\n",
    "            str_layer += l.__class__.__name__\n",
    "            str_layer +=\"\"\"',\\n\\t\\t'parameters':{\"\"\"\n",
    "            str_layer += \"\\n\\t\\t\\t'm_type': '\" + str(l.m_type) + \"',\"\n",
    "            str_layer += \"\\n\\t\\t\\t'm_function': '\" + str(l.m_function) + \"',\"\n",
    "            str_layer += \"\\n\\t\\t\\t'sub_SM_1': [\\n\\t\\t\"\n",
    "            j = 0\n",
    "            for sl in l.sub_SM_1:\n",
    "                str_layer += \"\"\"\\t\\t\\t{\n",
    "                    'class':'\"\"\"\n",
    "                str_layer += sl.__class__.__name__\n",
    "                str_layer +=\"\"\"',\\n\\t\\t\\t\\t\\t'parameters':\"\"\"\n",
    "                str_layer += str(sl.to_json())\n",
    "                str_layer += \"\"\"\\n\\t\\t\\t\\t}\"\"\"\n",
    "                \n",
    "                if(j < len(l.sub_SM_1)-1):\n",
    "                    str_layer += \"\"\",\"\"\"\n",
    "                    j+=1\n",
    "            str_layer += \"],\"        \n",
    "            str_layer += \"\\n\\t\\t\\t'sub_SM_2': [\\n\"\n",
    "            j = 0\n",
    "            for sl in l.sub_SM_2:\n",
    "                str_layer += \"\"\"\\t\\t\\t\\t{\n",
    "                    'class':'\"\"\"\n",
    "                str_layer += sl.__class__.__name__\n",
    "                str_layer +=\"\"\"',\\n\\t\\t\\t\\t\\t'parameters':\"\"\"\n",
    "                str_layer += str(sl.to_json())\n",
    "                str_layer += \"\"\"\\n\\t\\t\\t\\t}\"\"\"\n",
    "                \n",
    "                if(j < len(l.sub_SM_2)-1):\n",
    "                    str_layer += \"\"\",\"\"\"\n",
    "                    j+=1\n",
    "            \n",
    "            str_layer += \"]\" \n",
    "            str_layer += \"\"\"}\\n\\t\\t}\"\"\"\n",
    "            if( i < archi_size-1):\n",
    "                str_layer += \"\"\",\"\"\"\n",
    "                i+=1\n",
    "            str_layer = str_layer.replace(\"'\",\"\\\"\")\n",
    "        else: \n",
    "            str_layer = \"\"\"{\n",
    "        'class':'\"\"\"\n",
    "            str_layer += l.__class__.__name__\n",
    "            str_layer +=\"\"\"',\\n\\t\\t'parameters':\"\"\"\n",
    "            str_layer += str(l.to_json())\n",
    "            str_layer += \"\"\"\\n\\t}\"\"\"\n",
    "            if(i < archi_size-1):\n",
    "                str_layer += \"\"\",\"\"\"\n",
    "                i+=1\n",
    "            str_layer = str_layer.replace(\"'\",\"\\\"\")\n",
    "        \n",
    "        archi_file.write(str_layer)\n",
    "        \n",
    "    archi_file.write(\"\"\"\\n]\"\"\")\n",
    "    archi_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "id": "0c8cdea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# return value for kernel, stride and padding\n",
    "def optimisation_kernel_padding_stride(input_size):\n",
    "    padding=padding_value[random.randrange(2)]\n",
    "    \n",
    "    kernel = None\n",
    "    stride = None\n",
    "    \n",
    "    if (padding == \"valid\"):\n",
    "        \n",
    "        # stride <= input size\n",
    "        kernel_value_filtered = [value for value in kernel_value if value <= input_size]\n",
    "        kernel=kernel_value_filtered[random.randrange(len(kernel_value_filtered))]\n",
    "        \n",
    "        # stride <= kernel\n",
    "        stride = int(input_size/kernel)\n",
    "        if(stride > kernel): \n",
    "            stride_value_filtered = [value for value in stride_value if value <= kernel]\n",
    "            stride = stride_value_filtered[random.randrange(len(stride_value_filtered))]\n",
    "    else:\n",
    "        # Kernel <= output/2\n",
    "        if(output_size == 2):\n",
    "            kernel_value_filtered = [1,2]\n",
    "        elif(output_size == 1):\n",
    "            kernel_value_filtered = [1]\n",
    "        else: \n",
    "            kernel_value_filtered = [value for value in kernel_value if value <= math.ceil(input_size/2)]\n",
    "            \n",
    "        kernel=kernel_value_filtered[random.randrange(len(kernel_value_filtered))]\n",
    "        \n",
    "        # kernel >= stride\n",
    "        stride_value_filtered = [value for value in stride_value if value <= kernel]\n",
    "        stride = stride_value_filtered[random.randrange(len(stride_value_filtered))]\n",
    "    \n",
    "    return kernel, padding, stride"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "id": "162232b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#instantate the layer to add\n",
    "def construct_layer(layer, nb_filter_value, output_size, kernel = None, padding = None, stride = None):\n",
    "    \n",
    "    if (kernel == None): \n",
    "        kernel, padding, stride = optimisation_kernel_padding_stride(output_size)\n",
    "    \n",
    "    # if layer is Convolution\n",
    "    if(layer == Convolution):\n",
    "        add_layer = Convolution(\n",
    "            kernel=kernel, \n",
    "            padding=padding, \n",
    "            stride=stride, \n",
    "            nb_filter= nb_filter_value,\n",
    "            fct_activation=fct_activation_value[random.randrange(3)]\n",
    "        )\n",
    "    \n",
    "    # if is pooling\n",
    "    elif(layer == Pooling):\n",
    "        add_layer = Pooling(\n",
    "            op = op_value[random.randrange(2)],\n",
    "            kernel=kernel, \n",
    "            padding=padding, \n",
    "            stride=stride\n",
    "        )\n",
    "    #if is BN\n",
    "    elif(layer == BatchNormalisation):\n",
    "        add_layer = BatchNormalisation(epsilon = epsilon_value[random.randrange(len(epsilon_value))])\n",
    "        \n",
    "    #if dropout\n",
    "    elif(layer == Dropout):\n",
    "        add_layer = Dropout(dropout_rate = dropout_value[random.randrange(len(dropout_value))])\n",
    "    \n",
    "    # if Empty\n",
    "    elif(layer == Empty):\n",
    "        add_layer = Empty()\n",
    "            \n",
    "    else: \n",
    "        print(\"construct layer error\")\n",
    "    \n",
    "    return add_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "id": "e79d5d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add layer to architecture \n",
    "\n",
    "def addLayer(archi, add_layer, output_size):\n",
    "    \n",
    "    input_size = output_size\n",
    "    feature_extra = archi[1:]\n",
    "    \n",
    "    if( (add_layer == Dropout) | (add_layer == BatchNormalisation)):\n",
    "        archi.append(add_layer)\n",
    "        return 1, archi, output_size\n",
    "    \n",
    "    # add layer if the architecture is empty\n",
    "    elif(feature_extra == []):\n",
    "        archi.append(add_layer)\n",
    "        return 1, archi, calcul_output(input_size, add_layer)\n",
    "    \n",
    "    else :\n",
    "        \n",
    "        # if we couldn't reduce more\n",
    "        #if(input_size == 1):\n",
    "        #    return 0, archi, input_size\n",
    "        \n",
    "        # if the output size got more than 1 we can add new layer\n",
    "        if(output_size >= 1): \n",
    "            output_size = calcul_output(input_size, add_layer)\n",
    "            \n",
    "            # if output size got negate is that the layer we want to add is wrong\n",
    "            if(output_size < 1 ):\n",
    "                return -1, archi, input_size\n",
    "            # if output size is bigger than 0 we can add new layer and continue\n",
    "            elif(output_size > 0):\n",
    "                archi.append(add_layer)\n",
    "                return 1, archi, output_size\n",
    "            # this should not append\n",
    "            else:\n",
    "                return \"Somethink wrong\"\n",
    "    # this should not append\n",
    "    return \"Error\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "id": "d310cc8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the output \n",
    "# valid vs same\n",
    "def calcul_output(input_size, l):\n",
    "    output_size = 0\n",
    "    \n",
    "    if( (isinstance(l, Empty)) | (isinstance(l, BatchNormalisation)) | (isinstance(l, Dropout))):\n",
    "        return input_size\n",
    "    if(l.padding == \"valid\"):\n",
    "        kernel = l.kernel\n",
    "        stride = l.stride\n",
    "        while(input_size>=kernel):\n",
    "            input_size -= stride\n",
    "            output_size += 1   \n",
    "    else:\n",
    "        stride = l.stride\n",
    "        if(input_size%stride == 0):\n",
    "            output_size = int(input_size/stride)\n",
    "        else:\n",
    "            output_size = int(input_size/stride)+1\n",
    "    return output_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "id": "7d4bffe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_full_merge(sm1, sm2, output, nb_filter_value, j):\n",
    "    nb_layer_sm1 = random.randrange(len(nb_layer_value))\n",
    "    \n",
    "    output_sm1 = output\n",
    "    \n",
    "    for i in range(0, nb_layer_sm1):\n",
    "        if(len(sm2) > 2):\n",
    "            layer = getSub_SM_Layer(last_layer=sm1[len(sm1)-1], prev_last_layer=sm1[len(sm1)-2], tag=\"full\")\n",
    "        elif(len(sm2)>1):\n",
    "            layer = getSub_SM_Layer(last_layer=sm1[len(sm1)-1], tag=\"full\")\n",
    "        else:\n",
    "            layer = getSub_SM_Layer(tag=\"init\")\n",
    "        \n",
    "        kernel, padding, stride = optimisation_kernel_padding_stride(output_sm1)\n",
    "        add_layer = construct_layer(layer, nb_filter_value, output_sm1, padding=padding, stride=stride, kernel=kernel)\n",
    "        _, sm1, output = addLayer(sm1, add_layer, output_sm1)\n",
    "        \n",
    "        if (output < output_sm1):\n",
    "            if(isinstance(add_layer, Convolution)):\n",
    "                nb_filter_value = nb_filter_value*j\n",
    "                j=2\n",
    "                add_layer.nb_filter = nb_filter_value\n",
    "            sm1.pop()\n",
    "            _, sm1, output = addLayer(sm1, add_layer, output_sm1)\n",
    "            _, sm2, _ = addLayer(sm2, add_layer, output_sm1)\n",
    "        elif (output > output_sm1):\n",
    "            print(\"error output >\")\n",
    "        else:\n",
    "            if (isinstance(add_layer, BatchNormalisation)):\n",
    "                _, sm2, _ = addLayer(sm2, add_layer, output_sm1)\n",
    "            elif(output_sm1==1 and isinstance(add_layer, Convolution)):\n",
    "                _, sm2, _ = addLayer(sm2, add_layer, output_sm1)\n",
    "            else: print(\"nothing\")\n",
    "        \n",
    "        output_sm1 = output\n",
    "    print(output)\n",
    "    \n",
    "    return sm1, sm2, output_sm1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "id": "28c221c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_empty_merge(sm, output, nb_filter_value):\n",
    "    nb_layer = random.randrange(len(nb_layer_value))\n",
    "    for i in range(0, nb_layer):\n",
    "        \n",
    "        if(len(sm) > 2):\n",
    "            layer = getSub_SM_Layer(last_layer=sm[len(sm)-1], prev_last_layer=sm[len(sm)-2], tag=\"empty\")\n",
    "        elif(len(sm)>1):\n",
    "            layer = getSub_SM_Layer(last_layer=sm[len(sm)-1], tag=\"empty\")\n",
    "        else:\n",
    "            layer = getSub_SM_Layer(tag=\"init\")\n",
    "        \n",
    "        if(output == 2):\n",
    "            kernel_value_filtered = [1,2]\n",
    "        elif(output == 1):\n",
    "            kernel_value_filtered = [1]\n",
    "        else: \n",
    "            kernel_value_filtered = [value for value in kernel_value if value <= math.ceil(output/2)]\n",
    "\n",
    "        kernel=kernel_value_filtered[random.randrange(len(kernel_value_filtered))]\n",
    "        \n",
    "        \n",
    "        add_layer = construct_layer(layer, nb_filter_value, output, padding=\"same\", stride=1, kernel=kernel)\n",
    "        _, sm, output = addLayer(sm, add_layer, output)\n",
    "        \n",
    "    return sm, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "id": "57296599",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the merge part \n",
    "def build_merge(archi, output_size, nb_filter_value, j):\n",
    "    sub_sm1 = list()\n",
    "    sub_sm2 = list()\n",
    "    m_function = m_function_value[random.randrange(2)]\n",
    "    \n",
    "    m_type = merge_type[random.randrange(2)]\n",
    "    #m_type = merge_type[0]\n",
    "    \n",
    "    if(m_type == \"empty\"):\n",
    "        sub_sm1, output_size_sm1 = build_empty_merge(sub_sm1, output_size, nb_filter_value)\n",
    "        add_layer = construct_layer(Empty, nb_filter_value, output_size)\n",
    "        _, sub_sm2, output_size_sm2 = addLayer(sub_sm2, add_layer, output_size)\n",
    "    \n",
    "    else: \n",
    "        sub_sm1, sub_sm2, output_size = build_full_merge(sub_sm1, sub_sm2, output_size, nb_filter_value, j)\n",
    "    \n",
    "    m = Merge(m_type=m_type,\n",
    "              m_function=m_function,\n",
    "              sub_SM_1=sub_sm1,\n",
    "              sub_SM_2=sub_sm2)\n",
    "    \n",
    "    #if(equals_sm1_sm2(sub_sm1, sub_sm2)):\n",
    "        #build_merge(archi, output_size)\n",
    "        #print(\"equals\")\n",
    "        #return archi, output_size\n",
    "    #else: \n",
    "    archi.append(m)\n",
    "        \n",
    "    return archi , output_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "id": "c8d0e36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolution, Pooling, BatchNormalisation,  Dropout, Merge, Empty\n",
    "# Return list of layer compared to last layer for the sub state machine\n",
    "def getSub_SM_Layer(last_layer=None, prev_last_layer= None, tag=\"init\"):\n",
    "    list_layer = list()\n",
    "    if(tag == \"init\"):\n",
    "        list_layer = [Convolution, BatchNormalisation] # Merge\n",
    "    \n",
    "    elif(tag == \"empty\"):\n",
    "        if(isinstance(last_layer, Convolution)):\n",
    "                list_layer = [Convolution, BatchNormalisation, Dropout]\n",
    "        elif(isinstance(last_layer, BatchNormalisation)):\n",
    "            if(isinstance(prev_last_layer, Convolution)):\n",
    "                list_layer = [Convolution, Dropout]\n",
    "            else: list_layer = [Convolution]\n",
    "        else: list_layer = [Convolution]\n",
    "    else: \n",
    "        if(isinstance(last_layer, Convolution)):\n",
    "            list_layer = [Convolution, Pooling, BatchNormalisation, Dropout]\n",
    "        elif(isinstance(last_layer, Pooling)):\n",
    "            list_layer = [Convolution]\n",
    "        elif(isinstance(last_layer, BatchNormalisation)):\n",
    "            if(isinstance(prev_last_layer, Convolution)):\n",
    "                list_layer = [Convolution, Pooling, Dropout]\n",
    "            else: list_layer = [Convolution]\n",
    "        elif(isinstance(last_layer, Dropout)):\n",
    "            list_layer = [Pooling]\n",
    "    return list_layer[random.randrange(len(list_layer))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "id": "8de616e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolution, Pooling, BatchNormalisation, Dropout, Merge\n",
    "# Return the next layer compared to last layer\n",
    "def getlayer(last_layer, prev_last_layer):\n",
    "    list_layer = list()\n",
    "    if(isinstance(last_layer, InputLayer)):\n",
    "        list_layer = [Convolution, BatchNormalisation, Merge]\n",
    "    elif(isinstance(last_layer, Convolution)):\n",
    "        list_layer = [Convolution, Pooling, BatchNormalisation,  Dropout, Merge]\n",
    "    elif(isinstance(last_layer, Pooling)):\n",
    "        list_layer = [Convolution, Merge]\n",
    "    elif(isinstance(last_layer, BatchNormalisation)):\n",
    "        if(isinstance(prev_last_layer, Convolution)):\n",
    "            list_layer = [Convolution, Merge, Pooling, Dropout]\n",
    "        else: list_layer = [Convolution]\n",
    "    elif(isinstance(last_layer, Dropout)):\n",
    "        list_layer = [Pooling]\n",
    "    elif(isinstance(last_layer, Merge)):\n",
    "        list_layer = [Convolution, BatchNormalisation, Merge]\n",
    "    else: \n",
    "        print(\"getLayer error\")\n",
    "        print(last_layer)\n",
    "    return list_layer[random.randrange(len(list_layer))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440fafd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f018f5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1f571dfc",
   "metadata": {},
   "source": [
    "# Creation Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "id": "77f68bd3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nothing\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "output size : 1\n",
      "nb_filter : 1152\n",
      "code: 1\n",
      "<class '__main__.Flatten'>\n",
      "param : 1152\n",
      "=====\n",
      "iteration0\n",
      "nb_neurone : 230\n",
      "[InputLayer(shape=[28, 28, 1]), Merge(m_function='concatenate', m_type='full', sub_SM_1=[Convolution(fct_activation='relu', kernel=4, nb_filter=1, padding='same', stride=1), BatchNormalisation(epsilon=1.001e-05), Convolution(fct_activation='tanh', kernel=6, nb_filter=18, padding='valid', stride=4), BatchNormalisation(epsilon=1.1e-05), Convolution(fct_activation='selu', kernel=5, nb_filter=36, padding='valid', stride=1)], sub_SM_2=[BatchNormalisation(epsilon=1.001e-05), Convolution(fct_activation='tanh', kernel=6, nb_filter=18, padding='valid', stride=4), BatchNormalisation(epsilon=1.1e-05), Convolution(fct_activation='selu', kernel=5, nb_filter=36, padding='valid', stride=1)]), Merge(m_function='concatenate', m_type='empty', sub_SM_1=[BatchNormalisation(epsilon=1.1e-10), Convolution(fct_activation='relu', kernel=1, nb_filter=1, padding='same', stride=1), BatchNormalisation(epsilon=1.1e-10), Convolution(fct_activation='relu', kernel=2, nb_filter=1, padding='same', stride=1), BatchNormalisation(epsilon=0.001)], sub_SM_2=[Empty()]), Convolution(fct_activation='relu', kernel=2, nb_filter=18, padding='valid', stride=1), BatchNormalisation(epsilon=1.1e-10), Merge(m_function='concatenate', m_type='full', sub_SM_1=[Convolution(fct_activation='relu', kernel=1, nb_filter=18, padding='same', stride=1), Convolution(fct_activation='relu', kernel=1, nb_filter=18, padding='same', stride=1), Convolution(fct_activation='selu', kernel=1, nb_filter=18, padding='same', stride=1), BatchNormalisation(epsilon=1.1e-07)], sub_SM_2=[Convolution(fct_activation='relu', kernel=1, nb_filter=18, padding='same', stride=1), Convolution(fct_activation='relu', kernel=1, nb_filter=18, padding='same', stride=1), Convolution(fct_activation='selu', kernel=1, nb_filter=18, padding='same', stride=1), BatchNormalisation(epsilon=1.1e-07)]), BatchNormalisation(epsilon=1.001e-05), Convolution(fct_activation='tanh', kernel=1, nb_filter=36, padding='same', stride=1), Convolution(fct_activation='selu', kernel=1, nb_filter=72, padding='same', stride=1), BatchNormalisation(epsilon=1.001e-05), Convolution(fct_activation='relu', kernel=1, nb_filter=144, padding='valid', stride=1), Convolution(fct_activation='selu', kernel=1, nb_filter=288, padding='same', stride=1), BatchNormalisation(epsilon=1.1e-05), Pooling(kernel=1, op='avg', padding='same', stride=1), Convolution(fct_activation='selu', kernel=1, nb_filter=576, padding='valid', stride=1), Merge(m_function='concatenate', m_type='full', sub_SM_1=[Convolution(fct_activation='selu', kernel=1, nb_filter=576, padding='valid', stride=1), Convolution(fct_activation='tanh', kernel=1, nb_filter=576, padding='same', stride=1)], sub_SM_2=[Convolution(fct_activation='selu', kernel=1, nb_filter=576, padding='valid', stride=1), Convolution(fct_activation='tanh', kernel=1, nb_filter=576, padding='same', stride=1)]), Convolution(fct_activation='tanh', kernel=1, nb_filter=1152, padding='same', stride=1), Merge(m_function='add', m_type='empty', sub_SM_1=[BatchNormalisation(epsilon=1.1e-10)], sub_SM_2=[Empty()]), Merge(m_function='add', m_type='empty', sub_SM_1=[Convolution(fct_activation='relu', kernel=1, nb_filter=1152, padding='same', stride=1), Convolution(fct_activation='relu', kernel=1, nb_filter=1152, padding='same', stride=1), Dropout(dropout_rate=0.01), Convolution(fct_activation='relu', kernel=1, nb_filter=1152, padding='same', stride=1), BatchNormalisation(epsilon=1.1e-05)], sub_SM_2=[Empty()]), Merge(m_function='concatenate', m_type='full', sub_SM_1=[Convolution(fct_activation='relu', kernel=1, nb_filter=1152, padding='same', stride=1), BatchNormalisation(epsilon=0.001), Convolution(fct_activation='selu', kernel=1, nb_filter=1152, padding='same', stride=1), Convolution(fct_activation='selu', kernel=1, nb_filter=1152, padding='valid', stride=1)], sub_SM_2=[Convolution(fct_activation='relu', kernel=1, nb_filter=1152, padding='same', stride=1), BatchNormalisation(epsilon=0.001), Convolution(fct_activation='selu', kernel=1, nb_filter=1152, padding='same', stride=1), Convolution(fct_activation='selu', kernel=1, nb_filter=1152, padding='valid', stride=1)]), Merge(m_function='add', m_type='empty', sub_SM_1=[Convolution(fct_activation='selu', kernel=1, nb_filter=1152, padding='same', stride=1), Convolution(fct_activation='selu', kernel=1, nb_filter=1152, padding='same', stride=1), BatchNormalisation(epsilon=1.1e-07), Dropout(dropout_rate=0.001)], sub_SM_2=[Empty()]), Convolution(fct_activation='selu', kernel=1, nb_filter=2304, padding='same', stride=1), Dropout(dropout_rate=0.1), Pooling(kernel=1, op='max', padding='valid', stride=1), Merge(m_function='add', m_type='empty', sub_SM_1=[Convolution(fct_activation='relu', kernel=1, nb_filter=2304, padding='same', stride=1)], sub_SM_2=[Empty()]), Flatten(), Dense(fct_activation='relu', nb_neurones=230), Dense(fct_activation='softmax', nb_neurones=10)]\n",
      "archi_V4_test.json\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "architecture = list()\n",
    "\n",
    "nb_filter_value = 1 # init nb feature map\n",
    "output_size = 28\n",
    "\n",
    "# add input Layer\n",
    "architecture.append(InputLayer(shape=[28,28,1]))\n",
    "\n",
    "# add extraction feature (succession of Pooling/convolution)\n",
    "# Pooling can't be follow by a Pooling\n",
    "pooling = True\n",
    "code = 1 # we can add new layer\n",
    "j = 18\n",
    "last_layer = architecture[len(architecture)-1]\n",
    "\n",
    "while( ((isinstance(last_layer, Dropout)) | (isinstance(last_layer, BatchNormalisation))) | \n",
    "      (output_size == 28) | \n",
    "      ((stop(len(architecture),7)) & (code == 1))):\n",
    "    \n",
    "    layer = getlayer(architecture[len(architecture)-1], architecture[len(architecture)-2])\n",
    "    \n",
    "    \n",
    "    if (layer == Merge):\n",
    "        architecture, output_size = build_merge(architecture, output_size, nb_filter_value, j)\n",
    "    else:\n",
    "        if(layer == Convolution):\n",
    "            nb_filter_value = nb_filter_value*j\n",
    "            j=2\n",
    "        add_layer = construct_layer(layer, nb_filter_value, output_size)\n",
    "\n",
    "        code, architecture, output_size = addLayer(architecture, add_layer, output_size)\n",
    "        last_layer = architecture[len(architecture)-1]\n",
    "\n",
    "\n",
    "try:\n",
    "    nb_filter_value = architecture[len(architecture)-1]['nb_filter']\n",
    "except:\n",
    "    nb_filter_value = int(nb_filter_value/2)\n",
    "    \n",
    "\n",
    "print(\"output size : \" + str(output_size))\n",
    "print(\"nb_filter : \" + str(nb_filter_value))\n",
    "print(\"code: \" + str(code))\n",
    "\n",
    "clf_layer = classification[random.randrange(2)]\n",
    "print(clf_layer)\n",
    "if clf_layer == Flatten:\n",
    "    \n",
    "    # add flatten layer\n",
    "    architecture.append(Flatten())\n",
    "    \n",
    "    \n",
    "    # compute parameters\n",
    "    param = output_size*output_size*(nb_filter_value)\n",
    "    print('param : ' + str(param))\n",
    "\n",
    "\n",
    "    ## init values\n",
    "    nb=0\n",
    "    if (param >= 1000):\n",
    "        nb_neurones = int(param*20/100)\n",
    "        print(\"=====\")\n",
    "        print(\"iteration\" + str(nb))\n",
    "        print(\"nb_neurone : \" + str(nb_neurones))\n",
    "        dense = Dense(\n",
    "            nb_neurones = nb_neurones,\n",
    "            fct_activation = fct_activation_value[random.randrange(3)]\n",
    "        )\n",
    "        architecture.append(dense)\n",
    "        nb+=1\n",
    "        nb_neurones = int(nb_neurones*20/100)\n",
    "\n",
    "    else:\n",
    "        pourcent = random.uniform(10,90)\n",
    "        nb_neurones = int(param*pourcent/100)\n",
    "\n",
    "    while(nb_neurones >= 1000):\n",
    "        print(\"=====\")\n",
    "        print(\"iteration\" + str(nb))\n",
    "        print(\"nb_neurone : \" + str(nb_neurones))\n",
    "        dense = Dense(\n",
    "            nb_neurones = nb_neurones,\n",
    "            fct_activation = fct_activation_value[random.randrange(3)]\n",
    "        )\n",
    "        architecture.append(dense)\n",
    "        nb+=1\n",
    "        nb_neurones = int(nb_neurones*20/100)\n",
    "\n",
    "\n",
    "    ## add dense Layers\n",
    "    while(nb_neurones > nb_class*10):\n",
    "        print(\"=====\")\n",
    "        print(\"iteration\" + str(nb))\n",
    "        print(\"nb_neurone : \" + str(nb_neurones))\n",
    "\n",
    "        dense = Dense(\n",
    "            nb_neurones = nb_neurones,\n",
    "            fct_activation = fct_activation_value[random.randrange(3)]\n",
    "        )\n",
    "        architecture.append(dense)\n",
    "\n",
    "\n",
    "        #incrementation\n",
    "        pourcent = random.uniform(10,90)\n",
    "        nb+=1\n",
    "        nb_neurones = int(nb_neurones*pourcent/100)\n",
    "\n",
    "elif (clf_layer == GlobalPooling):\n",
    "    architecture.append(GlobalPooling(op=op_value[random.randrange(2)]))\n",
    "else: print(\"clf error\")\n",
    "\n",
    "\n",
    "last_dense = Dense(\n",
    "    nb_neurones = nb_class,\n",
    "    fct_activation=\"softmax\"           \n",
    ")\n",
    "\n",
    "architecture.append(last_dense)            \n",
    "        \n",
    "        \n",
    "print(architecture)\n",
    "\n",
    "\n",
    "# create file\n",
    "file_name = \"archi_V4_test.json\"\n",
    "print(file_name)\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "create_json_file(architecture, file_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ecda0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64740d29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
